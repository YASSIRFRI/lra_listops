{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-07T18:09:25.762817Z",
     "iopub.status.busy": "2024-12-07T18:09:25.762093Z",
     "iopub.status.idle": "2024-12-07T18:09:26.834175Z",
     "shell.execute_reply": "2024-12-07T18:09:26.833202Z",
     "shell.execute_reply.started": "2024-12-07T18:09:25.762774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/lra-listops/basic_test.tsv\n",
      "/kaggle/input/lra-listops/basic_train.tsv\n",
      "/kaggle/input/listops/test_d20s.tsv\n",
      "/kaggle/input/listops/train_d20s.tsv\n",
      "/kaggle/input/validation/basic_val.tsv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:14:19.240336Z",
     "iopub.status.busy": "2024-12-07T18:14:19.239573Z",
     "iopub.status.idle": "2024-12-07T18:14:44.305568Z",
     "shell.execute_reply": "2024-12-07T18:14:44.304641Z",
     "shell.execute_reply.started": "2024-12-07T18:14:19.240301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.40.2\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.40.2) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (2024.6.2)\n",
      "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.3\n",
      "    Uninstalling transformers-4.46.3:\n",
      "      Successfully uninstalled transformers-4.46.3\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.40.2\n"
     ]
    }
   ],
   "source": [
    "# Install the required version of transformers\n",
    "!pip install -U transformers==4.40.2\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import ReformerConfig, ReformerForSequenceClassification, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:15:09.642979Z",
     "iopub.status.busy": "2024-12-07T18:15:09.642482Z",
     "iopub.status.idle": "2024-12-07T18:15:10.599629Z",
     "shell.execute_reply": "2024-12-07T18:15:10.598599Z",
     "shell.execute_reply.started": "2024-12-07T18:15:09.642948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Target                                             Source\n",
      "0       6  ( ( ( ( ( [MAX ( ( ( ( ( ( [MED 4 ) 6 ) 6 ) 0 ...\n",
      "1       7   ( ( ( ( [SM ( ( ( [MED 6 ) 5 ) ] ) ) 1 ) 1 ) ] )\n",
      "2       4                 ( ( ( ( ( [MAX 3 ) 4 ) 3 ) 3 ) ] )\n",
      "3       0  ( ( ( ( ( [MIN 0 ) 0 ) ( ( ( [MAX 4 ) ( ( ( ( ...\n",
      "4       9  ( ( ( ( ( ( [SM ( ( ( ( [MIN 5 ) ( ( ( [MAX ( ...\n",
      "   Target                                             Source\n",
      "0       6  ( ( ( ( ( [MAX ( ( ( ( ( ( [MED 4 ) 6 ) 6 ) 0 ...\n",
      "1       7   ( ( ( ( [SM ( ( ( [MED 6 ) 5 ) ] ) ) 1 ) 1 ) ] )\n",
      "2       4                 ( ( ( ( ( [MAX 3 ) 4 ) 3 ) 3 ) ] )\n",
      "3       0  ( ( ( ( ( [MIN 0 ) 0 ) ( ( ( [MAX 4 ) ( ( ( ( ...\n",
      "4       9  ( ( ( ( ( ( [SM ( ( ( ( [MIN 5 ) ( ( ( [MAX ( ...\n",
      "Index(['Target', 'Source'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_file = '/kaggle/input/listops/train_d20s.tsv'\n",
    "test_file = '/kaggle/input/listops/test_d20s.tsv'\n",
    "\n",
    "train_df = pd.read_csv(train_file, sep='\\t', header=0)\n",
    "test_df = pd.read_csv(test_file, sep='\\t', header=0)\n",
    "print(train_df.head())\n",
    "\n",
    "train_df = pd.read_csv(train_file, sep='\\t', header=0)\n",
    "test_df = pd.read_csv(test_file, sep='\\t', header=0)\n",
    "print(train_df.head())\n",
    "print(train_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:15:26.180114Z",
     "iopub.status.busy": "2024-12-07T18:15:26.179417Z",
     "iopub.status.idle": "2024-12-07T18:15:26.184170Z",
     "shell.execute_reply": "2024-12-07T18:15:26.183202Z",
     "shell.execute_reply.started": "2024-12-07T18:15:26.180076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return ''.join(str(text).split())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:15:41.970707Z",
     "iopub.status.busy": "2024-12-07T18:15:41.970366Z",
     "iopub.status.idle": "2024-12-07T18:15:41.984700Z",
     "shell.execute_reply": "2024-12-07T18:15:41.983682Z",
     "shell.execute_reply.started": "2024-12-07T18:15:41.970679Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Target                                             Source\n",
      "0       6  ( ( ( ( ( [MAX ( ( ( ( ( ( [MED 4 ) 6 ) 6 ) 0 ...\n",
      "1       7   ( ( ( ( [SM ( ( ( [MED 6 ) 5 ) ] ) ) 1 ) 1 ) ] )\n",
      "2       4                 ( ( ( ( ( [MAX 3 ) 4 ) 3 ) 3 ) ] )\n",
      "3       0  ( ( ( ( ( [MIN 0 ) 0 ) ( ( ( [MAX 4 ) ( ( ( ( ...\n",
      "4       9  ( ( ( ( ( ( [SM ( ( ( ( [MIN 5 ) ( ( ( [MAX ( ...\n",
      "---------------\n",
      "73\n",
      "48\n",
      "34\n",
      "271\n",
      "501\n",
      "577\n",
      "72\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove any possible header rows included as data\n",
    "train_df = train_df[train_df['Target'] != 'Target']\n",
    "test_df = test_df[test_df['Target'] != 'Target']\n",
    "\n",
    "# Convert labels to integers\n",
    "train_df['Target'] = train_df['Target'].astype(int)\n",
    "test_df['Target'] = test_df['Target'].astype(int)\n",
    "print(train_df.head())\n",
    "print(\"---------------\")\n",
    "for i in range(8) : \n",
    "    print(len(train_df['Source'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:15:56.259995Z",
     "iopub.status.busy": "2024-12-07T18:15:56.259653Z",
     "iopub.status.idle": "2024-12-07T18:15:56.315058Z",
     "shell.execute_reply": "2024-12-07T18:15:56.314200Z",
     "shell.execute_reply.started": "2024-12-07T18:15:56.259965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sequence lengths:\n",
      "count    90000.000000\n",
      "mean       277.778200\n",
      "std        491.629979\n",
      "min          1.000000\n",
      "25%         49.000000\n",
      "50%         99.000000\n",
      "75%        259.000000\n",
      "max       7593.000000\n",
      "Name: Source, dtype: float64\n",
      "0     73\n",
      "1     48\n",
      "2     34\n",
      "3    271\n",
      "4    501\n",
      "Name: Source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Compute the lengths of the original sequences\n",
    "seqLengths= train_df['Source'].apply(lambda x: len(x))\n",
    "\n",
    "print(\"Training data sequence lengths:\")\n",
    "print(seqLengths.describe())\n",
    "\n",
    "print(seqLengths.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:16:14.221638Z",
     "iopub.status.busy": "2024-12-07T18:16:14.220756Z",
     "iopub.status.idle": "2024-12-07T18:16:14.312486Z",
     "shell.execute_reply": "2024-12-07T18:16:14.311561Z",
     "shell.execute_reply.started": "2024-12-07T18:16:14.221602Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sequence lengths:\n",
      "count    77650.000000\n",
      "mean       124.938416\n",
      "std        110.377955\n",
      "min          1.000000\n",
      "25%         43.000000\n",
      "50%         81.000000\n",
      "75%        165.000000\n",
      "max        511.000000\n",
      "Name: Source, dtype: float64\n",
      "0     73\n",
      "1     48\n",
      "2     34\n",
      "3    271\n",
      "4    501\n",
      "Name: Source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = train_df[train_df['Source'].apply(lambda x: len(x)) < 512]\n",
    "test_df = test_df[test_df['Source'].apply(lambda x: len(x)) < 512]\n",
    "\n",
    "\n",
    "\n",
    "# Check the filtered dataframe\n",
    "seqLengths = train_df['Source'].apply(lambda x: len(x))\n",
    "\n",
    "# Describe the sequence lengths\n",
    "print(\"Training data sequence lengths:\")\n",
    "print(seqLengths.describe())\n",
    "\n",
    "print(seqLengths.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:16:32.157228Z",
     "iopub.status.busy": "2024-12-07T18:16:32.156814Z",
     "iopub.status.idle": "2024-12-07T18:16:32.198118Z",
     "shell.execute_reply": "2024-12-07T18:16:32.197462Z",
     "shell.execute_reply.started": "2024-12-07T18:16:32.157179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the sequences to a text file for tokenizer training\n",
    "with open(\"listops_sequences.txt\", \"w\") as f:\n",
    "    for sequence in train_df[\"Source\"]:\n",
    "        f.write(sequence + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:16:51.278890Z",
     "iopub.status.busy": "2024-12-07T18:16:51.278563Z",
     "iopub.status.idle": "2024-12-07T18:16:52.400464Z",
     "shell.execute_reply": "2024-12-07T18:16:52.399814Z",
     "shell.execute_reply.started": "2024-12-07T18:16:51.278860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialize a WordLevel tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Set the pre-tokenization strategy\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Prepare a trainer with special tokens\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "\n",
    "# Train the tokenizer on your text file\n",
    "tokenizer.train([\"listops_sequences.txt\"], trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"custom_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:17:17.510489Z",
     "iopub.status.busy": "2024-12-07T18:17:17.510143Z",
     "iopub.status.idle": "2024-12-07T18:17:17.516464Z",
     "shell.execute_reply": "2024-12-07T18:17:17.515419Z",
     "shell.execute_reply.started": "2024-12-07T18:17:17.510459Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=23, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the custom tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:17:33.136082Z",
     "iopub.status.busy": "2024-12-07T18:17:33.135719Z",
     "iopub.status.idle": "2024-12-07T18:17:33.142657Z",
     "shell.execute_reply": "2024-12-07T18:17:33.141628Z",
     "shell.execute_reply.started": "2024-12-07T18:17:33.136049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LRADataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels.astype(int)  # Ensure labels are integers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        # Tokenize and encode the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:21:15.990636Z",
     "iopub.status.busy": "2024-12-07T18:21:15.990277Z",
     "iopub.status.idle": "2024-12-07T18:21:15.997444Z",
     "shell.execute_reply": "2024-12-07T18:21:15.996608Z",
     "shell.execute_reply.started": "2024-12-07T18:21:15.990595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = LRADataset(\n",
    "        texts=df['Source'].to_numpy(),\n",
    "        labels=df['Target'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Create data loaders\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:21:21.163665Z",
     "iopub.status.busy": "2024-12-07T18:21:21.163316Z",
     "iopub.status.idle": "2024-12-07T18:21:21.213759Z",
     "shell.execute_reply": "2024-12-07T18:21:21.212882Z",
     "shell.execute_reply.started": "2024-12-07T18:21:21.163628Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77650\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "num_labels = train_df[\"Source\"].nunique()\n",
    "print(num_labels)\n",
    "print(tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:22:38.541652Z",
     "iopub.status.busy": "2024-12-07T18:22:38.540832Z",
     "iopub.status.idle": "2024-12-07T18:22:38.580479Z",
     "shell.execute_reply": "2024-12-07T18:22:38.579822Z",
     "shell.execute_reply.started": "2024-12-07T18:22:38.541618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import MobileBertConfig, MobileBertForSequenceClassification\n",
    "\n",
    "#  MobileBERT for the ListOps task\n",
    "config = MobileBertConfig(\n",
    "    vocab_size=23,\n",
    "    hidden_size=32,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=128,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    num_labels=10,\n",
    "    pad_token_id=0\n",
    ")\n",
    "\n",
    "# Initialize model and use single GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MobileBertForSequenceClassification(config)\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:22:41.365889Z",
     "iopub.status.busy": "2024-12-07T18:22:41.365260Z",
     "iopub.status.idle": "2024-12-07T18:22:41.379449Z",
     "shell.execute_reply": "2024-12-07T18:22:41.378448Z",
     "shell.execute_reply.started": "2024-12-07T18:22:41.365853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)  # Wrap the model for multiple GPUs\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "EPOCHS = 5\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps // 10,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:23:52.151473Z",
     "iopub.status.busy": "2024-12-07T18:23:52.150833Z",
     "iopub.status.idle": "2024-12-07T18:23:52.161120Z",
     "shell.execute_reply": "2024-12-07T18:23:52.160216Z",
     "shell.execute_reply.started": "2024-12-07T18:23:52.151439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss  #\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "        # Update metrics\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        # Take mean if loss is not already reduced\n",
    "        if loss.dim() > 0:\n",
    "            loss = loss.mean()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            # Update metrics\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            if loss.dim() > 0:\n",
    "                loss = loss.mean()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:23:57.238337Z",
     "iopub.status.busy": "2024-12-07T18:23:57.237745Z",
     "iopub.status.idle": "2024-12-07T19:18:05.052886Z",
     "shell.execute_reply": "2024-12-07T19:18:05.052101Z",
     "shell.execute_reply.started": "2024-12-07T18:23:57.238300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.2690013328639487 accuracy 0.12798454603992274\n",
      "Val   loss 2.309552396889086 accuracy 0.09759203519333179\n",
      "Epoch 2/5\n",
      "----------\n",
      "Train loss 2.3084985762604284 accuracy 0.10588538312942691\n",
      "Val   loss 2.305334810195146 accuracy 0.09770780273211391\n",
      "Epoch 3/5\n",
      "----------\n",
      "Train loss 2.3055060945825057 accuracy 0.11103670315518352\n",
      "Val   loss 2.305973263360836 accuracy 0.0985181755035888\n",
      "Epoch 4/5\n",
      "----------\n",
      "Train loss 2.303169927403501 accuracy 0.11280103026400515\n",
      "Val   loss 2.3029526266786786 accuracy 0.11102106969205834\n",
      "Epoch 5/5\n",
      "----------\n",
      "Train loss 2.301761214613018 accuracy 0.1148486799742434\n",
      "Val   loss 2.3004655679066977 accuracy 0.11102106969205834\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'train_acc': [],\n",
    "    'train_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        test_data_loader,\n",
    "        loss_fn,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "\n",
    "    history['train_acc'].append(train_acc.cpu().numpy())\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc.cpu().numpy())\n",
    "    history['val_loss'].append(val_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6213643,
     "sourceId": 10079404,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6217911,
     "sourceId": 10085161,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6239887,
     "sourceId": 10113799,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
