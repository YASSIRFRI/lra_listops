{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10079404,"sourceType":"datasetVersion","datasetId":6213643},{"sourceId":10113626,"sourceType":"datasetVersion","datasetId":6239759},{"sourceId":10121160,"sourceType":"datasetVersion","datasetId":6245231}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Install the required version of transformers\n!pip install -U transformers==4.40.2\n\n# Import necessary libraries\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nimport os\nfrom transformers import ReformerConfig, ReformerForSequenceClassification, AutoTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport torch.nn as nn\n","metadata":{"id":"bnXKpCNPf9Sk","outputId":"ecc5d481-aa5b-4ea6-a956-2e49ddca5e12","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:25.546854Z","iopub.execute_input":"2024-12-07T17:28:25.547698Z","iopub.status.idle":"2024-12-07T17:28:33.903909Z","shell.execute_reply.started":"2024-12-07T17:28:25.547655Z","shell.execute_reply":"2024-12-07T17:28:33.902991Z"},"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers==4.40.2 in /opt/conda/lib/python3.10/site-packages (4.40.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.40.2) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (2024.6.2)\n","output_type":"stream"}],"execution_count":97},{"cell_type":"markdown","source":"## Importing Data","metadata":{}},{"cell_type":"code","source":"# Define the file paths\ntrain_file = '/kaggle/input/depth-20/train_d20s.tsv'\ntest_file = '/kaggle/input/depth-20/test_d20s.tsv'\n\n# Load the data\ntrain_df = pd.read_csv(train_file, sep='\\t', header=0)\ntest_df = pd.read_csv(test_file, sep='\\t', header=0)\n# Preview the data\nprint(train_df.head())\n\n# Load the data\ntrain_df = pd.read_csv(train_file, sep='\\t', header=0)\ntest_df = pd.read_csv(test_file, sep='\\t', header=0)\n# Preview the data\nprint(train_df.head())\nprint(train_df.columns)\n","metadata":{"id":"s2NiNMIVf9Sp","outputId":"6bf37d1b-9cc5-44a8-b2bd-52bf9fce1cac","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:33.906367Z","iopub.execute_input":"2024-12-07T17:28:33.907148Z","iopub.status.idle":"2024-12-07T17:28:34.576265Z","shell.execute_reply.started":"2024-12-07T17:28:33.907103Z","shell.execute_reply":"2024-12-07T17:28:34.575319Z"}},"outputs":[{"name":"stdout","text":"   Target                                             Source\n0       6  ( ( ( ( ( [MAX ( ( ( ( ( ( [MED 4 ) 6 ) 6 ) 0 ...\n1       7   ( ( ( ( [SM ( ( ( [MED 6 ) 5 ) ] ) ) 1 ) 1 ) ] )\n2       4                 ( ( ( ( ( [MAX 3 ) 4 ) 3 ) 3 ) ] )\n3       0  ( ( ( ( ( [MIN 0 ) 0 ) ( ( ( [MAX 4 ) ( ( ( ( ...\n4       9  ( ( ( ( ( ( [SM ( ( ( ( [MIN 5 ) ( ( ( [MAX ( ...\n   Target                                             Source\n0       6  ( ( ( ( ( [MAX ( ( ( ( ( ( [MED 4 ) 6 ) 6 ) 0 ...\n1       7   ( ( ( ( [SM ( ( ( [MED 6 ) 5 ) ] ) ) 1 ) 1 ) ] )\n2       4                 ( ( ( ( ( [MAX 3 ) 4 ) 3 ) 3 ) ] )\n3       0  ( ( ( ( ( [MIN 0 ) 0 ) ( ( ( [MAX 4 ) ( ( ( ( ...\n4       9  ( ( ( ( ( ( [SM ( ( ( ( [MIN 5 ) ( ( ( [MAX ( ...\nIndex(['Target', 'Source'], dtype='object')\n","output_type":"stream"}],"execution_count":98},{"cell_type":"markdown","source":"## Basic data cleaning && exploration","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    return ''.join(str(text).split())\n\n# Apply the cleaning function to the 'Source' column\n#train_df['Source'] = train_df['Source'].apply(clean_text)\n#test_df['Source'] = test_df['Source'].apply(clean_text)\n","metadata":{"id":"RoggwnV9f9Sq","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:34.577247Z","iopub.execute_input":"2024-12-07T17:28:34.577528Z","iopub.status.idle":"2024-12-07T17:28:34.581961Z","shell.execute_reply.started":"2024-12-07T17:28:34.577503Z","shell.execute_reply":"2024-12-07T17:28:34.580955Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"\n# Remove any possible header rows included as data\ntrain_df = train_df[train_df['Target'] != 'Target']\ntest_df = test_df[test_df['Target'] != 'Target']\n\n# Convert labels to integers\ntrain_df['Target'] = train_df['Target'].astype(int)\ntest_df['Target'] = test_df['Target'].astype(int)\nprint(train_df.head())\nprint(\"---------------\")\nfor i in range(8) : \n    print(len(train_df['Source'][i]))","metadata":{"id":"rEG3i6q-f9Sr","outputId":"10390f55-b653-41ff-fed5-466c7a71c427","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:34.584755Z","iopub.execute_input":"2024-12-07T17:28:34.585166Z","iopub.status.idle":"2024-12-07T17:28:34.600574Z","shell.execute_reply.started":"2024-12-07T17:28:34.585129Z","shell.execute_reply":"2024-12-07T17:28:34.599763Z"}},"outputs":[{"name":"stdout","text":"   Target                                             Source\n0       6  ( ( ( ( ( [MAX ( ( ( ( ( ( [MED 4 ) 6 ) 6 ) 0 ...\n1       7   ( ( ( ( [SM ( ( ( [MED 6 ) 5 ) ] ) ) 1 ) 1 ) ] )\n2       4                 ( ( ( ( ( [MAX 3 ) 4 ) 3 ) 3 ) ] )\n3       0  ( ( ( ( ( [MIN 0 ) 0 ) ( ( ( [MAX 4 ) ( ( ( ( ...\n4       9  ( ( ( ( ( ( [SM ( ( ( ( [MIN 5 ) ( ( ( [MAX ( ...\n---------------\n73\n48\n34\n271\n501\n577\n72\n172\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"# Compute the lengths of the original sequences\nseqLengths= train_df['Source'].apply(lambda x: len(x))\n\n# Describe the sequence lengths\nprint(\"Training data sequence lengths:\")\nprint(seqLengths.describe())\n\nprint(seqLengths.head())\n\n\n\n","metadata":{"id":"_lRw41IPf9Ss","outputId":"a913a18a-8930-4ce7-99b3-b92b6884e116","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:34.601847Z","iopub.execute_input":"2024-12-07T17:28:34.602512Z","iopub.status.idle":"2024-12-07T17:28:34.651048Z","shell.execute_reply.started":"2024-12-07T17:28:34.602453Z","shell.execute_reply":"2024-12-07T17:28:34.650230Z"}},"outputs":[{"name":"stdout","text":"Training data sequence lengths:\ncount    90000.000000\nmean       277.778200\nstd        491.629979\nmin          1.000000\n25%         49.000000\n50%         99.000000\n75%        259.000000\nmax       7593.000000\nName: Source, dtype: float64\n0     73\n1     48\n2     34\n3    271\n4    501\nName: Source, dtype: int64\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"import pandas as pd\n\n# Filter the training and test datasets based on the condition\ntrain_df = train_df[train_df['Source'].apply(lambda x: len(x)) < 512]\ntest_df = test_df[test_df['Source'].apply(lambda x: len(x)) < 512]\n\n# # Randomly sample a percentage of the filtered train data\n# train_df = train_df.sample(frac=1, random_state=42)  # Set random_state for reproducibility\n\n# Check the filtered dataframe\nseqLengths = train_df['Source'].apply(lambda x: len(x))\n\n# Describe the sequence lengths\nprint(\"Training data sequence lengths:\")\nprint(seqLengths.describe())\n\nprint(seqLengths.head())\n","metadata":{"id":"l3GY0_E7f9St","outputId":"b2dbf190-0076-43c7-e477-55730679624d","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:34.652017Z","iopub.execute_input":"2024-12-07T17:28:34.652255Z","iopub.status.idle":"2024-12-07T17:28:34.738741Z","shell.execute_reply.started":"2024-12-07T17:28:34.652232Z","shell.execute_reply":"2024-12-07T17:28:34.737704Z"}},"outputs":[{"name":"stdout","text":"Training data sequence lengths:\ncount    77650.000000\nmean       124.938416\nstd        110.377955\nmin          1.000000\n25%         43.000000\n50%         81.000000\n75%        165.000000\nmax        511.000000\nName: Source, dtype: float64\n0     73\n1     48\n2     34\n3    271\n4    501\nName: Source, dtype: int64\n","output_type":"stream"}],"execution_count":102},{"cell_type":"code","source":"# Save the sequences to a text file for tokenizer training\nwith open(\"listops_sequences.txt\", \"w\") as f:\n    for sequence in train_df[\"Source\"]:\n        f.write(sequence + \"\\n\")\n","metadata":{"id":"epaeuELUf9Su","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:34.739861Z","iopub.execute_input":"2024-12-07T17:28:34.740167Z","iopub.status.idle":"2024-12-07T17:28:34.787811Z","shell.execute_reply.started":"2024-12-07T17:28:34.740141Z","shell.execute_reply":"2024-12-07T17:28:34.787118Z"}},"outputs":[],"execution_count":103},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# Initialize a WordLevel tokenizer\ntokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n\n# Set the pre-tokenization strategy\ntokenizer.pre_tokenizer = Whitespace()\n\n# Prepare a trainer with special tokens\ntrainer = WordLevelTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n\n# Train the tokenizer on your text file\ntokenizer.train([\"listops_sequences.txt\"], trainer)\n\n# Save the tokenizer\ntokenizer.save(\"custom_tokenizer.json\")\n","metadata":{"id":"4sjbfMHNf9Sv","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:34.788773Z","iopub.execute_input":"2024-12-07T17:28:34.789029Z","iopub.status.idle":"2024-12-07T17:28:35.911096Z","shell.execute_reply.started":"2024-12-07T17:28:34.789004Z","shell.execute_reply":"2024-12-07T17:28:35.910402Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\n\n# Load the custom tokenizer\ntokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nprint(tokenizer)","metadata":{"id":"-zK6YG-Uf9Sv","outputId":"45587c65-36f7-47ed-85f3-ad5416e148f6","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:35.912040Z","iopub.execute_input":"2024-12-07T17:28:35.912342Z","iopub.status.idle":"2024-12-07T17:28:35.917420Z","shell.execute_reply.started":"2024-12-07T17:28:35.912295Z","shell.execute_reply":"2024-12-07T17:28:35.916557Z"}},"outputs":[{"name":"stdout","text":"PreTrainedTokenizerFast(name_or_path='', vocab_size=23, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n","output_type":"stream"}],"execution_count":105},{"cell_type":"code","source":"class LRADataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels.astype(int)  # Ensure labels are integers\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item]\n\n        # Tokenize and encode the text\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"id":"ZdJ7y6hcf9Sw","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:35.919868Z","iopub.execute_input":"2024-12-07T17:28:35.920151Z","iopub.status.idle":"2024-12-07T17:28:35.930322Z","shell.execute_reply.started":"2024-12-07T17:28:35.920125Z","shell.execute_reply":"2024-12-07T17:28:35.929550Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    dataset = LRADataset(\n        texts=df['Source'].to_numpy(),\n        labels=df['Target'].to_numpy(),\n        tokenizer=tokenizer,\n        max_len=max_len\n    )\n\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=10\n    )\n\n# Parameters\nMAX_LEN = 512\nBATCH_SIZE = 8\n\n# Create data loaders\ntrain_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n","metadata":{"id":"pRX285REf9Sw","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:35.931354Z","iopub.execute_input":"2024-12-07T17:28:35.931604Z","iopub.status.idle":"2024-12-07T17:28:35.943499Z","shell.execute_reply.started":"2024-12-07T17:28:35.931580Z","shell.execute_reply":"2024-12-07T17:28:35.942731Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"num_labels = train_df[\"Source\"].nunique()\nprint(num_labels)\nprint(tokenizer.vocab_size)\n","metadata":{"id":"i2eA5Lhgf9Sx","outputId":"d10260f8-7d5d-4a7c-c8ad-8da0fd1f8654","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:35.944610Z","iopub.execute_input":"2024-12-07T17:28:35.944861Z","iopub.status.idle":"2024-12-07T17:28:35.987682Z","shell.execute_reply.started":"2024-12-07T17:28:35.944839Z","shell.execute_reply":"2024-12-07T17:28:35.986906Z"}},"outputs":[{"name":"stdout","text":"77650\n23\n","output_type":"stream"}],"execution_count":108},{"cell_type":"code","source":"from transformers import GPT2Config, GPT2ForSequenceClassification\n\n# Custom GPT-2 configuration for ListOps task\nconfig = GPT2Config(\n    vocab_size=23,  # Adjust for the ListOps vocabulary\n    n_positions=512,  # Max sequence length (optimized for ListOps)\n    n_embd=32,  # Reduced hidden state dimensionality for efficiency\n    n_layer=4,  # Fewer Transformer layers for smaller model size\n    n_head=4,  # Fewer attention heads for smaller memory footprint\n    n_inner=512,  # Size of inner feed-forward layer\n    activation_function=\"gelu_new\",  # GELU activation function\n    resid_pdrop=0.1,  # Dropout probability for residual connections\n    embd_pdrop=0.1,  # Dropout probability for embeddings\n    attn_pdrop=0.1,  # Dropout probability for attention probabilities\n    layer_norm_epsilon=1e-5,  # Epsilon for layer normalization\n    initializer_range=0.02,  # Standard deviation for weight initialization\n    summary_type=\"cls_index\",  # Summarization strategy for classification\n    summary_use_proj=True,  # Use a projection layer for summarization\n    summary_activation=None,  # Activation function for summarization\n    summary_proj_to_labels=True,  # Project summary to labels\n    summary_first_dropout=0.1,  # Dropout probability for the summarization layer\n    scale_attn_weights=True,  # Scale attention weights\n    use_cache=True,  # Enable caching\n    bos_token_id=0,  # Adjusted for ListOps vocabulary\n    eos_token_id=1,  # Adjusted for ListOps vocabulary\n    scale_attn_by_inverse_layer_idx=False,  # No scaling by inverse layer index\n    reorder_and_upcast_attn=False,  # Standard attention reordering\n    num_labels=10, \n    pad_token_id=0# Number of classes in the ListOps task\n)\n\n# Initialize the GPT-2 model for sequence classification\nmodel = GPT2ForSequenceClassification(config)\n\n# Print the model configuration for verification\nprint(model.config)","metadata":{"id":"O9vpnzKqf9Sx","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:35.988806Z","iopub.execute_input":"2024-12-07T17:28:35.989622Z","iopub.status.idle":"2024-12-07T17:28:36.012852Z","shell.execute_reply.started":"2024-12-07T17:28:35.989581Z","shell.execute_reply":"2024-12-07T17:28:36.012142Z"}},"outputs":[{"name":"stdout","text":"GPT2Config {\n  \"activation_function\": \"gelu_new\",\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 0,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 1,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\",\n    \"7\": \"LABEL_7\",\n    \"8\": \"LABEL_8\",\n    \"9\": \"LABEL_9\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6,\n    \"LABEL_7\": 7,\n    \"LABEL_8\": 8,\n    \"LABEL_9\": 9\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_embd\": 32,\n  \"n_head\": 4,\n  \"n_inner\": 512,\n  \"n_layer\": 4,\n  \"n_positions\": 512,\n  \"pad_token_id\": 0,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.40.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 23\n}\n\n","output_type":"stream"}],"execution_count":109},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model)  # Wrap the model for multiple GPUs\n\nmodel = model.to(device)\n\n# Optimizer and scheduler\nEPOCHS = 5\noptimizer = AdamW(model.parameters(), lr=1e-3)\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=total_steps // 10,\n    num_training_steps=total_steps\n)\n\n# Loss function\nloss_fn = torch.nn.CrossEntropyLoss().to(device)\n","metadata":{"id":"R-EhrN8Qf9Sy","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:36.013776Z","iopub.execute_input":"2024-12-07T17:28:36.014031Z","iopub.status.idle":"2024-12-07T17:28:36.027124Z","shell.execute_reply.started":"2024-12-07T17:28:36.014004Z","shell.execute_reply":"2024-12-07T17:28:36.026179Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n    model.train()\n    losses = []\n    correct_predictions = 0\n\n    for batch in data_loader:\n        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n        labels = batch[\"labels\"].to(device, non_blocking=True)\n\n        # Forward pass\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        loss = outputs.loss\n        logits = outputs.logits\n        _, preds = torch.max(logits, dim=1)\n\n        # Update metrics\n        correct_predictions += torch.sum(preds == labels)\n        losses.append(loss.item())\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n\n\ndef eval_model(model, data_loader, loss_fn, device):\n    model.eval()\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n            labels = batch[\"labels\"].to(device, non_blocking=True)\n\n            # Forward pass\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            loss = outputs.loss\n            logits = outputs.logits\n            _, preds = torch.max(logits, dim=1)\n\n            # Update metrics\n            correct_predictions += torch.sum(preds == labels)\n            losses.append(loss.item())\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n","metadata":{"id":"lBgjLgLbf9Sz","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:36.028430Z","iopub.execute_input":"2024-12-07T17:28:36.028763Z","iopub.status.idle":"2024-12-07T17:28:36.038108Z","shell.execute_reply.started":"2024-12-07T17:28:36.028725Z","shell.execute_reply":"2024-12-07T17:28:36.037292Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"history = {\n    'train_acc': [],\n    'train_loss': [],\n    'val_acc': [],\n    'val_loss': []\n}\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n\n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler\n    )\n\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n\n    val_acc, val_loss = eval_model(\n        model,\n        test_data_loader,\n        loss_fn,\n        device\n    )\n\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n\n    history['train_acc'].append(train_acc.cpu().numpy())\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc.cpu().numpy())\n    history['val_loss'].append(val_loss)\n","metadata":{"id":"6j57p2O3f9Sz","outputId":"f49cd757-75b3-467c-d725-00059ccfed91","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:28:36.039158Z","iopub.execute_input":"2024-12-07T17:28:36.039415Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Saving the trained model","metadata":{}},{"cell_type":"code","source":"# Specify the directory to save the model\noutput_dir = './my_model_listops_reduced_best_scheduled/'\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Save the trained model and tokenizer\nmodel_to_save = model.module if hasattr(model, 'module') else model\nmodel_to_save.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model saved to {output_dir}\")\n","metadata":{"id":"o1Yjmz2_f9S0","trusted":true},"outputs":[],"execution_count":null}]}