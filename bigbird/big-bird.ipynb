{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.status.busy": "2024-12-03T18:05:16.801109Z",
     "iopub.status.idle": "2024-12-03T18:05:16.801731Z",
     "shell.execute_reply": "2024-12-03T18:05:16.801449Z",
     "shell.execute_reply.started": "2024-12-03T18:05:16.801418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pip installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:05:16.802811Z",
     "iopub.status.idle": "2024-12-03T18:05:16.803438Z",
     "shell.execute_reply": "2024-12-03T18:05:16.803152Z",
     "shell.execute_reply.started": "2024-12-03T18:05:16.803120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install transformers torch pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigbird on test dataset as train\n",
    "(because train dataset requires a lot of computation power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:43:12.532251Z",
     "iopub.status.busy": "2024-12-03T19:43:12.531897Z",
     "iopub.status.idle": "2024-12-03T19:52:00.077583Z",
     "shell.execute_reply": "2024-12-03T19:52:00.076652Z",
     "shell.execute_reply.started": "2024-12-03T19:43:12.532219Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/1129453338.py:143: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 1/10 Training:   0%|          | 0/436 [00:00<?, ?it/s]/tmp/ipykernel_30/1129453338.py:161: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n",
      "Epoch 1/10 Training: 100%|██████████| 436/436 [00:31<00:00, 13.86it/s]\n",
      "Validation: 100%|██████████| 432/432 [00:22<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.6003\n",
      "Validation Loss: 2.2578\n",
      "Validation Accuracy: 0.1922\n",
      "Best model saved with validation accuracy: 0.1922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training: 100%|██████████| 436/436 [00:31<00:00, 14.05it/s]\n",
      "Validation: 100%|██████████| 432/432 [00:21<00:00, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.5728\n",
      "Validation Loss: 2.2762\n",
      "Validation Accuracy: 0.1691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training: 100%|██████████| 436/436 [00:31<00:00, 13.94it/s]\n",
      "Validation: 100%|██████████| 432/432 [00:21<00:00, 20.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.5726\n",
      "Validation Loss: 2.2552\n",
      "Validation Accuracy: 0.1922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training: 100%|██████████| 436/436 [00:31<00:00, 13.99it/s]\n",
      "Validation: 100%|██████████| 432/432 [00:21<00:00, 20.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.5702\n",
      "Validation Loss: 2.2722\n",
      "Validation Accuracy: 0.1691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Training: 100%|██████████| 436/436 [00:31<00:00, 13.99it/s]\n",
      "Validation: 100%|██████████| 432/432 [00:21<00:00, 20.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.5685\n",
      "Validation Loss: 2.2496\n",
      "Validation Accuracy: 0.1922\n",
      "Model checkpoint saved at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Training: 100%|██████████| 436/436 [00:31<00:00, 14.00it/s]\n",
      "Validation: 100%|██████████| 432/432 [00:21<00:00, 20.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.5673\n",
      "Validation Loss: 2.2869\n",
      "Validation Accuracy: 0.1691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Training: 100%|██████████| 436/436 [00:31<00:00, 14.00it/s]\n",
      "Validation: 100%|██████████| 432/432 [00:21<00:00, 20.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.5681\n",
      "Validation Loss: 2.2576\n",
      "Validation Accuracy: 0.1691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Training: 100%|██████████| 436/436 [00:31<00:00, 14.00it/s]\n",
      "Validation: 100%|██████████| 432/432 [00:21<00:00, 20.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.5666\n",
      "Validation Loss: 2.2644\n",
      "Validation Accuracy: 0.1691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Training: 100%|██████████| 436/436 [00:31<00:00, 13.95it/s]\n",
      "Validation: 100%|██████████| 432/432 [00:21<00:00, 20.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.5688\n",
      "Validation Loss: 2.2525\n",
      "Validation Accuracy: 0.1691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Training: 100%|██████████| 436/436 [00:31<00:00, 13.98it/s]\n",
      "Validation: 100%|██████████| 432/432 [00:21<00:00, 20.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.5667\n",
      "Validation Loss: 2.2587\n",
      "Validation Accuracy: 0.1922\n",
      "Model checkpoint saved at epoch 10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BigBirdConfig, BigBirdModel\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ListOpsDataset(Dataset):\n",
    "    def __init__(self, file_path, max_length=4096): \n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        \n",
    "        # Filter sequences by length and keep original spacing\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        for text, label in zip(df['Source'], df['Target']):\n",
    "            if len(text) < 10000:  # Only keep sequences less than 10000 characters\n",
    "                self.texts.append(text)\n",
    "                self.labels.append(label)\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.vocab = {\n",
    "            'PAD': 0, '[': 1, ']': 2, 'SM': 3, 'MAX': 4,\n",
    "            'MIN': 5, 'MED': 6, '0': 7, '1': 8, '2': 9,\n",
    "            '3': 10, '4': 11, '5': 12, '6': 13, '7': 14,\n",
    "            '8': 15, '9': 16, '(': 17, ')': 18, ' ': 19, '\\t': 20\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = [self.vocab[char] for char in text if char in self.vocab]\n",
    "        \n",
    "        # Create attention mask and truncate/pad\n",
    "        attention_mask = [1] * min(len(tokens), self.max_length)\n",
    "        tokens = tokens[:self.max_length]\n",
    "        \n",
    "        if len(tokens) < self.max_length:\n",
    "            padding_length = self.max_length - len(tokens)\n",
    "            tokens = tokens + [self.vocab['PAD']] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length\n",
    "            \n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class BigBirdListOps(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.config = BigBirdConfig(\n",
    "            hidden_size=512,      \n",
    "            num_attention_heads=8,\n",
    "            intermediate_size=2048,\n",
    "            num_hidden_layers=6,  \n",
    "            vocab_size=21,  \n",
    "            max_position_embeddings=4096,\n",
    "            attention_type=\"block_sparse\",\n",
    "            block_size=64,        \n",
    "            num_random_blocks=3,   \n",
    "        )\n",
    "        self.bigbird = BigBirdModel(self.config)\n",
    "        self.classifier = nn.Linear(512, num_classes) \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bigbird(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "        return logits\n",
    "\n",
    "def validate_model(model, val_loader, device, criterion):\n",
    "    \"\"\"\n",
    "    Validate the model on a validation dataset\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to validate\n",
    "        val_loader (DataLoader): Validation data loader\n",
    "        device (torch.device): Device to run validation on\n",
    "        criterion (nn.Module): Loss function\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Average validation loss and accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_val_loss = total_loss / len(val_loader)\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_val_loss, val_accuracy\n",
    "\n",
    "def train_model(train_path, val_path, batch_size=8, num_epochs=10, learning_rate=1e-4, save_interval=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ListOpsDataset(train_path)\n",
    "    val_dataset = ListOpsDataset(val_path)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = BigBirdListOps().to(device)\n",
    "    \n",
    "    # Use gradient accumulation and mixed precision to reduce memory usage\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Training loop\n",
    "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\")):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Use mixed precision training\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss / 4  # Gradient accumulation\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            if (i + 1) % 4 == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        avg_val_loss, val_accuracy = validate_model(model, val_loader, device, criterion)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Save best model based on validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_accuracy': best_val_accuracy,\n",
    "            }, 'best_bigbird_listops.pth')\n",
    "            print(f'Best model saved with validation accuracy: {best_val_accuracy:.4f}')\n",
    "        \n",
    "        # Save periodic checkpoints\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_accuracy': val_accuracy,\n",
    "            }, f'bigbird_listops_epoch_{epoch+1}.pth')\n",
    "            print(f'Model checkpoint saved at epoch {epoch+1}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update paths for train and validation datasets\n",
    "    train_model(\n",
    "        train_path='/kaggle/input/lra-listops/basic_test.tsv',\n",
    "        val_path='/kaggle/input/validation/basic_val.tsv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigbird model on 10 % of train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T18:05:29.048135Z",
     "iopub.status.busy": "2024-12-03T18:05:29.047297Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Epoch 1/5: 100%|██████████| 4800/4800 [08:48<00:00,  9.08it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train - Loss: 2.2567, Accuracy: 0.1635, F1: 0.0460\n",
      "Val   - Loss: 2.2697, Accuracy: 0.1550, F1: 0.0416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 4800/4800 [08:48<00:00,  9.08it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train - Loss: 2.2590, Accuracy: 0.1643, F1: 0.0464\n",
      "Val   - Loss: 2.2715, Accuracy: 0.1600, F1: 0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 4800/4800 [08:48<00:00,  9.08it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train - Loss: 2.2562, Accuracy: 0.1643, F1: 0.0464\n",
      "Val   - Loss: 2.2655, Accuracy: 0.1600, F1: 0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 4800/4800 [08:47<00:00,  9.10it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train - Loss: 2.2570, Accuracy: 0.1635, F1: 0.0460\n",
      "Val   - Loss: 2.2819, Accuracy: 0.1550, F1: 0.0416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 4800/4800 [08:47<00:00,  9.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BigBirdConfig, BigBirdModel, PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def create_custom_tokenizer(train_path):\n",
    "    df = pd.read_csv(train_path, sep='\\t')\n",
    "    train_texts = df['Source'].iloc[:len(df)//10].tolist()\n",
    "    \n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "    \n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=23,\n",
    "        special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]\n",
    "    )\n",
    "    \n",
    "    tokenizer.train_from_iterator(train_texts, trainer)\n",
    "    wrapped_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "    wrapped_tokenizer.pad_token = \"[PAD]\"\n",
    "    return wrapped_tokenizer\n",
    "\n",
    "class ListOpsDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=8192):\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        df = df.iloc[:len(df)//10]\n",
    "        self.texts = df['Source'].tolist()\n",
    "        self.labels = df['Target'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'][0],\n",
    "            'attention_mask': encoding['attention_mask'][0],\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class BigBirdListOps(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.config = BigBirdConfig(\n",
    "            hidden_size=8,\n",
    "            num_attention_heads=2,\n",
    "            intermediate_size=512,\n",
    "            num_hidden_layers=2,\n",
    "            vocab_size=vocab_size,\n",
    "            max_position_embeddings=8192,\n",
    "            attention_type=\"block_sparse\",\n",
    "            block_size=64,\n",
    "            num_random_blocks=2\n",
    "        )\n",
    "        self.bigbird = BigBirdModel(self.config)\n",
    "        self.classifier = nn.Linear(8, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bigbird(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "        return logits\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def train_model(train_path, val_path, batch_size=2, num_epochs=5, learning_rate=1e-4):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = create_custom_tokenizer(train_path)\n",
    "    \n",
    "    train_dataset = ListOpsDataset(train_path, tokenizer)\n",
    "    val_dataset = ListOpsDataset(val_path, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = BigBirdListOps(vocab_size=tokenizer.vocab_size).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_metrics = evaluate(model, train_loader, criterion, device)\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Train - Loss: {train_metrics[\"loss\"]:.4f}, Accuracy: {train_metrics[\"accuracy\"]:.4f}, F1: {train_metrics[\"f1\"]:.4f}')\n",
    "        print(f'Val   - Loss: {val_metrics[\"loss\"]:.4f}, Accuracy: {val_metrics[\"accuracy\"]:.4f}, F1: {val_metrics[\"f1\"]:.4f}')\n",
    "        \n",
    "        torch.save(model.state_dict(), f'bigbird_listops_epoch_{epoch+1}.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        train_path='/kaggle/input/lra-listops/basic_train.tsv',\n",
    "        val_path='/kaggle/input/validation/basic_val.tsv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN with attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T20:05:52.081681Z",
     "iopub.status.busy": "2024-12-03T20:05:52.081351Z",
     "iopub.status.idle": "2024-12-03T20:20:17.875318Z",
     "shell.execute_reply": "2024-12-03T20:20:17.874410Z",
     "shell.execute_reply.started": "2024-12-03T20:05:52.081654Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/2700439763.py:112: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "\n",
      "Epoch 1/10:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A/tmp/ipykernel_30/2700439763.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "\n",
      "Epoch 1/10:   2%|▏         | 1/55 [00:00<00:53,  1.01it/s]\u001b[A\n",
      "Epoch 1/10:   4%|▎         | 2/55 [00:01<00:49,  1.06it/s]\u001b[A\n",
      "Epoch 1/10:   5%|▌         | 3/55 [00:02<00:47,  1.08it/s]\u001b[A\n",
      "Epoch 1/10:   7%|▋         | 4/55 [00:03<00:46,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:   9%|▉         | 5/55 [00:04<00:45,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  11%|█         | 6/55 [00:05<00:44,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  13%|█▎        | 7/55 [00:06<00:43,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  15%|█▍        | 8/55 [00:07<00:42,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  16%|█▋        | 9/55 [00:08<00:41,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  18%|█▊        | 10/55 [00:09<00:40,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  20%|██        | 11/55 [00:10<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  22%|██▏       | 12/55 [00:10<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  24%|██▎       | 13/55 [00:11<00:38,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  25%|██▌       | 14/55 [00:12<00:37,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  27%|██▋       | 15/55 [00:13<00:36,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  29%|██▉       | 16/55 [00:14<00:36,  1.07it/s]\u001b[A\n",
      "Epoch 1/10:  31%|███       | 17/55 [00:15<00:35,  1.07it/s]\u001b[A\n",
      "Epoch 1/10:  33%|███▎      | 18/55 [00:16<00:34,  1.08it/s]\u001b[A\n",
      "Epoch 1/10:  35%|███▍      | 19/55 [00:17<00:33,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  36%|███▋      | 20/55 [00:18<00:32,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  38%|███▊      | 21/55 [00:19<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  40%|████      | 22/55 [00:20<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  42%|████▏     | 23/55 [00:21<00:29,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  44%|████▎     | 24/55 [00:21<00:28,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  45%|████▌     | 25/55 [00:22<00:27,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  47%|████▋     | 26/55 [00:23<00:26,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  49%|████▉     | 27/55 [00:24<00:25,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  51%|█████     | 28/55 [00:25<00:24,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  53%|█████▎    | 29/55 [00:26<00:23,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  55%|█████▍    | 30/55 [00:27<00:23,  1.06it/s]\u001b[A\n",
      "Epoch 1/10:  56%|█████▋    | 31/55 [00:28<00:22,  1.05it/s]\u001b[A\n",
      "Epoch 1/10:  58%|█████▊    | 32/55 [00:29<00:21,  1.06it/s]\u001b[A\n",
      "Epoch 1/10:  60%|██████    | 33/55 [00:30<00:20,  1.08it/s]\u001b[A\n",
      "Epoch 1/10:  62%|██████▏   | 34/55 [00:31<00:19,  1.08it/s]\u001b[A\n",
      "Epoch 1/10:  64%|██████▎   | 35/55 [00:32<00:18,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  65%|██████▌   | 36/55 [00:33<00:17,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  67%|██████▋   | 37/55 [00:34<00:16,  1.08it/s]\u001b[A\n",
      "Epoch 1/10:  69%|██████▉   | 38/55 [00:34<00:15,  1.08it/s]\u001b[A\n",
      "Epoch 1/10:  71%|███████   | 39/55 [00:35<00:14,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  73%|███████▎  | 40/55 [00:36<00:13,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  75%|███████▍  | 41/55 [00:37<00:12,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  76%|███████▋  | 42/55 [00:38<00:11,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  78%|███████▊  | 43/55 [00:39<00:10,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  80%|████████  | 44/55 [00:40<00:10,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  82%|████████▏ | 45/55 [00:41<00:09,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  84%|████████▎ | 46/55 [00:42<00:08,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  85%|████████▌ | 47/55 [00:43<00:07,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  87%|████████▋ | 48/55 [00:44<00:06,  1.10it/s]\u001b[A\n",
      "Epoch 1/10:  89%|████████▉ | 49/55 [00:45<00:05,  1.08it/s]\u001b[A\n",
      "Epoch 1/10:  91%|█████████ | 50/55 [00:45<00:04,  1.08it/s]\u001b[A\n",
      "Epoch 1/10:  93%|█████████▎| 51/55 [00:46<00:03,  1.08it/s]\u001b[A\n",
      "Epoch 1/10:  95%|█████████▍| 52/55 [00:47<00:02,  1.08it/s]\u001b[A\n",
      "Epoch 1/10:  96%|█████████▋| 53/55 [00:48<00:01,  1.09it/s]\u001b[A\n",
      "Epoch 1/10:  98%|█████████▊| 54/55 [00:49<00:00,  1.09it/s]\u001b[A\n",
      "Epoch 1/10: 100%|██████████| 55/55 [00:50<00:00,  1.09it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Training Loss: 2.2607\n",
      "Validation Loss: 2.2361\n",
      "Validation Accuracy: 19.22%\n",
      "New best model saved with validation accuracy: 19.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2/10:   2%|▏         | 1/55 [00:00<00:49,  1.08it/s]\u001b[A\n",
      "Epoch 2/10:   4%|▎         | 2/55 [00:01<00:48,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:   5%|▌         | 3/55 [00:02<00:47,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:   7%|▋         | 4/55 [00:03<00:46,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:   9%|▉         | 5/55 [00:04<00:45,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  11%|█         | 6/55 [00:05<00:44,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  13%|█▎        | 7/55 [00:06<00:43,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  15%|█▍        | 8/55 [00:07<00:43,  1.09it/s]\u001b[A\n",
      "Epoch 2/10:  16%|█▋        | 9/55 [00:08<00:42,  1.09it/s]\u001b[A\n",
      "Epoch 2/10:  18%|█▊        | 10/55 [00:09<00:41,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  20%|██        | 11/55 [00:10<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  22%|██▏       | 12/55 [00:10<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  24%|██▎       | 13/55 [00:11<00:38,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  25%|██▌       | 14/55 [00:12<00:37,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  27%|██▋       | 15/55 [00:13<00:36,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  29%|██▉       | 16/55 [00:14<00:35,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  31%|███       | 17/55 [00:15<00:34,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  33%|███▎      | 18/55 [00:16<00:33,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  35%|███▍      | 19/55 [00:17<00:32,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  36%|███▋      | 20/55 [00:18<00:31,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  38%|███▊      | 21/55 [00:19<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  40%|████      | 22/55 [00:19<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  42%|████▏     | 23/55 [00:20<00:29,  1.09it/s]\u001b[A\n",
      "Epoch 2/10:  44%|████▎     | 24/55 [00:21<00:28,  1.09it/s]\u001b[A\n",
      "Epoch 2/10:  45%|████▌     | 25/55 [00:22<00:27,  1.09it/s]\u001b[A\n",
      "Epoch 2/10:  47%|████▋     | 26/55 [00:23<00:26,  1.08it/s]\u001b[A\n",
      "Epoch 2/10:  49%|████▉     | 27/55 [00:24<00:25,  1.09it/s]\u001b[A\n",
      "Epoch 2/10:  51%|█████     | 28/55 [00:25<00:24,  1.09it/s]\u001b[A\n",
      "Epoch 2/10:  53%|█████▎    | 29/55 [00:26<00:23,  1.09it/s]\u001b[A\n",
      "Epoch 2/10:  55%|█████▍    | 30/55 [00:27<00:22,  1.09it/s]\u001b[A\n",
      "Epoch 2/10:  56%|█████▋    | 31/55 [00:28<00:21,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  58%|█████▊    | 32/55 [00:29<00:20,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  60%|██████    | 33/55 [00:30<00:19,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  62%|██████▏   | 34/55 [00:30<00:18,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  64%|██████▎   | 35/55 [00:31<00:18,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  65%|██████▌   | 36/55 [00:32<00:17,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  67%|██████▋   | 37/55 [00:33<00:16,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  69%|██████▉   | 38/55 [00:34<00:15,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  71%|███████   | 39/55 [00:35<00:14,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  73%|███████▎  | 40/55 [00:36<00:13,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  75%|███████▍  | 41/55 [00:37<00:12,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  76%|███████▋  | 42/55 [00:38<00:11,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  78%|███████▊  | 43/55 [00:39<00:10,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  80%|████████  | 44/55 [00:40<00:10,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  82%|████████▏ | 45/55 [00:40<00:09,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  84%|████████▎ | 46/55 [00:41<00:08,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  85%|████████▌ | 47/55 [00:42<00:07,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  87%|████████▋ | 48/55 [00:43<00:06,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  89%|████████▉ | 49/55 [00:44<00:05,  1.11it/s]\u001b[A\n",
      "Epoch 2/10:  91%|█████████ | 50/55 [00:45<00:04,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  93%|█████████▎| 51/55 [00:46<00:03,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  95%|█████████▍| 52/55 [00:47<00:02,  1.10it/s]\u001b[A\n",
      "Epoch 2/10:  96%|█████████▋| 53/55 [00:48<00:01,  1.09it/s]\u001b[A\n",
      "Epoch 2/10:  98%|█████████▊| 54/55 [00:49<00:00,  1.09it/s]\u001b[A\n",
      "Epoch 2/10: 100%|██████████| 55/55 [00:49<00:00,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Training Loss: 2.2439\n",
      "Validation Loss: 2.2393\n",
      "Validation Accuracy: 16.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3/10:   2%|▏         | 1/55 [00:00<00:49,  1.09it/s]\u001b[A\n",
      "Epoch 3/10:   4%|▎         | 2/55 [00:01<00:48,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:   5%|▌         | 3/55 [00:02<00:47,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:   7%|▋         | 4/55 [00:03<00:46,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:   9%|▉         | 5/55 [00:04<00:45,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  11%|█         | 6/55 [00:05<00:44,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  13%|█▎        | 7/55 [00:06<00:43,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  15%|█▍        | 8/55 [00:07<00:42,  1.11it/s]\u001b[A\n",
      "Epoch 3/10:  16%|█▋        | 9/55 [00:08<00:41,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  18%|█▊        | 10/55 [00:09<00:40,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  20%|██        | 11/55 [00:09<00:39,  1.11it/s]\u001b[A\n",
      "Epoch 3/10:  22%|██▏       | 12/55 [00:10<00:38,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  24%|██▎       | 13/55 [00:11<00:38,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  25%|██▌       | 14/55 [00:12<00:37,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  27%|██▋       | 15/55 [00:13<00:36,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  29%|██▉       | 16/55 [00:14<00:35,  1.11it/s]\u001b[A\n",
      "Epoch 3/10:  31%|███       | 17/55 [00:15<00:34,  1.11it/s]\u001b[A\n",
      "Epoch 3/10:  33%|███▎      | 18/55 [00:16<00:33,  1.11it/s]\u001b[A\n",
      "Epoch 3/10:  35%|███▍      | 19/55 [00:17<00:32,  1.11it/s]\u001b[A\n",
      "Epoch 3/10:  36%|███▋      | 20/55 [00:18<00:31,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  38%|███▊      | 21/55 [00:19<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  40%|████      | 22/55 [00:19<00:29,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  42%|████▏     | 23/55 [00:20<00:29,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  44%|████▎     | 24/55 [00:21<00:28,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  45%|████▌     | 25/55 [00:22<00:27,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  47%|████▋     | 26/55 [00:23<00:26,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  49%|████▉     | 27/55 [00:24<00:25,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  51%|█████     | 28/55 [00:25<00:24,  1.11it/s]\u001b[A\n",
      "Epoch 3/10:  53%|█████▎    | 29/55 [00:26<00:23,  1.11it/s]\u001b[A\n",
      "Epoch 3/10:  55%|█████▍    | 30/55 [00:27<00:22,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  56%|█████▋    | 31/55 [00:28<00:21,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  58%|█████▊    | 32/55 [00:29<00:21,  1.09it/s]\u001b[A\n",
      "Epoch 3/10:  60%|██████    | 33/55 [00:29<00:20,  1.09it/s]\u001b[A\n",
      "Epoch 3/10:  62%|██████▏   | 34/55 [00:30<00:19,  1.09it/s]\u001b[A\n",
      "Epoch 3/10:  64%|██████▎   | 35/55 [00:31<00:18,  1.09it/s]\u001b[A\n",
      "Epoch 3/10:  65%|██████▌   | 36/55 [00:32<00:17,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  67%|██████▋   | 37/55 [00:33<00:16,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  69%|██████▉   | 38/55 [00:34<00:15,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  71%|███████   | 39/55 [00:35<00:14,  1.09it/s]\u001b[A\n",
      "Epoch 3/10:  73%|███████▎  | 40/55 [00:36<00:13,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  75%|███████▍  | 41/55 [00:37<00:12,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  76%|███████▋  | 42/55 [00:38<00:11,  1.09it/s]\u001b[A\n",
      "Epoch 3/10:  78%|███████▊  | 43/55 [00:39<00:10,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  80%|████████  | 44/55 [00:39<00:10,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  82%|████████▏ | 45/55 [00:40<00:09,  1.08it/s]\u001b[A\n",
      "Epoch 3/10:  84%|████████▎ | 46/55 [00:41<00:08,  1.09it/s]\u001b[A\n",
      "Epoch 3/10:  85%|████████▌ | 47/55 [00:42<00:07,  1.09it/s]\u001b[A\n",
      "Epoch 3/10:  87%|████████▋ | 48/55 [00:43<00:06,  1.09it/s]\u001b[A\n",
      "Epoch 3/10:  89%|████████▉ | 49/55 [00:44<00:05,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  91%|█████████ | 50/55 [00:45<00:04,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  93%|█████████▎| 51/55 [00:46<00:03,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  95%|█████████▍| 52/55 [00:47<00:02,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  96%|█████████▋| 53/55 [00:48<00:01,  1.10it/s]\u001b[A\n",
      "Epoch 3/10:  98%|█████████▊| 54/55 [00:49<00:00,  1.10it/s]\u001b[A\n",
      "Epoch 3/10: 100%|██████████| 55/55 [00:49<00:00,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Training Loss: 2.2429\n",
      "Validation Loss: 2.2375\n",
      "Validation Accuracy: 16.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4/10:   2%|▏         | 1/55 [00:00<00:48,  1.11it/s]\u001b[A\n",
      "Epoch 4/10:   4%|▎         | 2/55 [00:01<00:48,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:   5%|▌         | 3/55 [00:02<00:47,  1.09it/s]\u001b[A\n",
      "Epoch 4/10:   7%|▋         | 4/55 [00:03<00:46,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:   9%|▉         | 5/55 [00:04<00:45,  1.09it/s]\u001b[A\n",
      "Epoch 4/10:  11%|█         | 6/55 [00:05<00:45,  1.08it/s]\u001b[A\n",
      "Epoch 4/10:  13%|█▎        | 7/55 [00:06<00:44,  1.08it/s]\u001b[A\n",
      "Epoch 4/10:  15%|█▍        | 8/55 [00:07<00:43,  1.09it/s]\u001b[A\n",
      "Epoch 4/10:  16%|█▋        | 9/55 [00:08<00:41,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  18%|█▊        | 10/55 [00:09<00:40,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  20%|██        | 11/55 [00:10<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  22%|██▏       | 12/55 [00:10<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  24%|██▎       | 13/55 [00:11<00:38,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  25%|██▌       | 14/55 [00:12<00:37,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  27%|██▋       | 15/55 [00:13<00:36,  1.11it/s]\u001b[A\n",
      "Epoch 4/10:  29%|██▉       | 16/55 [00:14<00:35,  1.09it/s]\u001b[A\n",
      "Epoch 4/10:  31%|███       | 17/55 [00:15<00:34,  1.09it/s]\u001b[A\n",
      "Epoch 4/10:  33%|███▎      | 18/55 [00:16<00:33,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  35%|███▍      | 19/55 [00:17<00:32,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  36%|███▋      | 20/55 [00:18<00:31,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  38%|███▊      | 21/55 [00:19<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  40%|████      | 22/55 [00:20<00:29,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  42%|████▏     | 23/55 [00:20<00:28,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  44%|████▎     | 24/55 [00:21<00:28,  1.11it/s]\u001b[A\n",
      "Epoch 4/10:  45%|████▌     | 25/55 [00:22<00:27,  1.11it/s]\u001b[A\n",
      "Epoch 4/10:  47%|████▋     | 26/55 [00:23<00:26,  1.11it/s]\u001b[A\n",
      "Epoch 4/10:  49%|████▉     | 27/55 [00:24<00:25,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  51%|█████     | 28/55 [00:25<00:24,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  53%|█████▎    | 29/55 [00:26<00:23,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  55%|█████▍    | 30/55 [00:27<00:22,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  56%|█████▋    | 31/55 [00:28<00:21,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  58%|█████▊    | 32/55 [00:29<00:20,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  60%|██████    | 33/55 [00:30<00:19,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  62%|██████▏   | 34/55 [00:30<00:19,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  64%|██████▎   | 35/55 [00:31<00:18,  1.11it/s]\u001b[A\n",
      "Epoch 4/10:  65%|██████▌   | 36/55 [00:32<00:17,  1.11it/s]\u001b[A\n",
      "Epoch 4/10:  67%|██████▋   | 37/55 [00:33<00:16,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  69%|██████▉   | 38/55 [00:34<00:15,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  71%|███████   | 39/55 [00:35<00:14,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  73%|███████▎  | 40/55 [00:36<00:13,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  75%|███████▍  | 41/55 [00:37<00:12,  1.09it/s]\u001b[A\n",
      "Epoch 4/10:  76%|███████▋  | 42/55 [00:38<00:11,  1.09it/s]\u001b[A\n",
      "Epoch 4/10:  78%|███████▊  | 43/55 [00:39<00:10,  1.09it/s]\u001b[A\n",
      "Epoch 4/10:  80%|████████  | 44/55 [00:40<00:10,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  82%|████████▏ | 45/55 [00:40<00:09,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  84%|████████▎ | 46/55 [00:41<00:08,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  85%|████████▌ | 47/55 [00:42<00:07,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  87%|████████▋ | 48/55 [00:43<00:06,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  89%|████████▉ | 49/55 [00:44<00:05,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  91%|█████████ | 50/55 [00:45<00:04,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  93%|█████████▎| 51/55 [00:46<00:03,  1.10it/s]\u001b[A\n",
      "Epoch 4/10:  95%|█████████▍| 52/55 [00:47<00:02,  1.09it/s]\u001b[A\n",
      "Epoch 4/10:  96%|█████████▋| 53/55 [00:48<00:01,  1.09it/s]\u001b[A\n",
      "Epoch 4/10:  98%|█████████▊| 54/55 [00:49<00:00,  1.09it/s]\u001b[A\n",
      "Epoch 4/10: 100%|██████████| 55/55 [00:50<00:00,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Training Loss: 2.2422\n",
      "Validation Loss: 2.2360\n",
      "Validation Accuracy: 16.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5/10:   2%|▏         | 1/55 [00:00<00:48,  1.11it/s]\u001b[A\n",
      "Epoch 5/10:   4%|▎         | 2/55 [00:01<00:48,  1.09it/s]\u001b[A\n",
      "Epoch 5/10:   5%|▌         | 3/55 [00:02<00:47,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:   7%|▋         | 4/55 [00:03<00:46,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:   9%|▉         | 5/55 [00:04<00:45,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  11%|█         | 6/55 [00:05<00:44,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  13%|█▎        | 7/55 [00:06<00:43,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  15%|█▍        | 8/55 [00:07<00:42,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  16%|█▋        | 9/55 [00:08<00:41,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  18%|█▊        | 10/55 [00:09<00:41,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  20%|██        | 11/55 [00:10<00:40,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  22%|██▏       | 12/55 [00:10<00:38,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  24%|██▎       | 13/55 [00:11<00:38,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  25%|██▌       | 14/55 [00:12<00:37,  1.11it/s]\u001b[A\n",
      "Epoch 5/10:  27%|██▋       | 15/55 [00:13<00:36,  1.09it/s]\u001b[A\n",
      "Epoch 5/10:  29%|██▉       | 16/55 [00:14<00:35,  1.09it/s]\u001b[A\n",
      "Epoch 5/10:  31%|███       | 17/55 [00:15<00:34,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  33%|███▎      | 18/55 [00:16<00:33,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  35%|███▍      | 19/55 [00:17<00:32,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  36%|███▋      | 20/55 [00:18<00:31,  1.09it/s]\u001b[A\n",
      "Epoch 5/10:  38%|███▊      | 21/55 [00:19<00:31,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  40%|████      | 22/55 [00:20<00:29,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  42%|████▏     | 23/55 [00:20<00:29,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  44%|████▎     | 24/55 [00:21<00:28,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  45%|████▌     | 25/55 [00:22<00:27,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  47%|████▋     | 26/55 [00:23<00:26,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  49%|████▉     | 27/55 [00:24<00:25,  1.11it/s]\u001b[A\n",
      "Epoch 5/10:  51%|█████     | 28/55 [00:25<00:24,  1.11it/s]\u001b[A\n",
      "Epoch 5/10:  53%|█████▎    | 29/55 [00:26<00:23,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  55%|█████▍    | 30/55 [00:27<00:22,  1.11it/s]\u001b[A\n",
      "Epoch 5/10:  56%|█████▋    | 31/55 [00:28<00:21,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  58%|█████▊    | 32/55 [00:29<00:20,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  60%|██████    | 33/55 [00:30<00:20,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  62%|██████▏   | 34/55 [00:30<00:19,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  64%|██████▎   | 35/55 [00:31<00:18,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  65%|██████▌   | 36/55 [00:32<00:17,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  67%|██████▋   | 37/55 [00:33<00:16,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  69%|██████▉   | 38/55 [00:34<00:15,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  71%|███████   | 39/55 [00:35<00:14,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  73%|███████▎  | 40/55 [00:36<00:13,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  75%|███████▍  | 41/55 [00:37<00:12,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  76%|███████▋  | 42/55 [00:38<00:11,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  78%|███████▊  | 43/55 [00:39<00:10,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  80%|████████  | 44/55 [00:40<00:09,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  82%|████████▏ | 45/55 [00:40<00:09,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  84%|████████▎ | 46/55 [00:41<00:08,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  85%|████████▌ | 47/55 [00:42<00:07,  1.09it/s]\u001b[A\n",
      "Epoch 5/10:  87%|████████▋ | 48/55 [00:43<00:06,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  89%|████████▉ | 49/55 [00:44<00:05,  1.10it/s]\u001b[A\n",
      "Epoch 5/10:  91%|█████████ | 50/55 [00:45<00:04,  1.07it/s]\u001b[A\n",
      "Epoch 5/10:  93%|█████████▎| 51/55 [00:46<00:03,  1.08it/s]\u001b[A\n",
      "Epoch 5/10:  95%|█████████▍| 52/55 [00:47<00:02,  1.09it/s]\u001b[A\n",
      "Epoch 5/10:  96%|█████████▋| 53/55 [00:48<00:01,  1.08it/s]\u001b[A\n",
      "Epoch 5/10:  98%|█████████▊| 54/55 [00:49<00:00,  1.09it/s]\u001b[A\n",
      "Epoch 5/10: 100%|██████████| 55/55 [00:50<00:00,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Training Loss: 2.2432\n",
      "Validation Loss: 2.2356\n",
      "Validation Accuracy: 19.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6/10:   2%|▏         | 1/55 [00:00<00:48,  1.11it/s]\u001b[A\n",
      "Epoch 6/10:   4%|▎         | 2/55 [00:01<00:48,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:   5%|▌         | 3/55 [00:02<00:47,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:   7%|▋         | 4/55 [00:03<00:46,  1.11it/s]\u001b[A\n",
      "Epoch 6/10:   9%|▉         | 5/55 [00:04<00:45,  1.11it/s]\u001b[A\n",
      "Epoch 6/10:  11%|█         | 6/55 [00:05<00:44,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  13%|█▎        | 7/55 [00:06<00:43,  1.11it/s]\u001b[A\n",
      "Epoch 6/10:  15%|█▍        | 8/55 [00:07<00:42,  1.11it/s]\u001b[A\n",
      "Epoch 6/10:  16%|█▋        | 9/55 [00:08<00:41,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  18%|█▊        | 10/55 [00:09<00:40,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  20%|██        | 11/55 [00:09<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  22%|██▏       | 12/55 [00:10<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  24%|██▎       | 13/55 [00:11<00:38,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  25%|██▌       | 14/55 [00:12<00:37,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  27%|██▋       | 15/55 [00:13<00:36,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  29%|██▉       | 16/55 [00:14<00:35,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  31%|███       | 17/55 [00:15<00:34,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  33%|███▎      | 18/55 [00:16<00:33,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  35%|███▍      | 19/55 [00:17<00:32,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  36%|███▋      | 20/55 [00:18<00:31,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  38%|███▊      | 21/55 [00:19<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  40%|████      | 22/55 [00:19<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  42%|████▏     | 23/55 [00:20<00:29,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  44%|████▎     | 24/55 [00:21<00:29,  1.06it/s]\u001b[A\n",
      "Epoch 6/10:  45%|████▌     | 25/55 [00:22<00:28,  1.07it/s]\u001b[A\n",
      "Epoch 6/10:  47%|████▋     | 26/55 [00:23<00:26,  1.08it/s]\u001b[A\n",
      "Epoch 6/10:  49%|████▉     | 27/55 [00:24<00:26,  1.07it/s]\u001b[A\n",
      "Epoch 6/10:  51%|█████     | 28/55 [00:25<00:25,  1.08it/s]\u001b[A\n",
      "Epoch 6/10:  53%|█████▎    | 29/55 [00:26<00:23,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  55%|█████▍    | 30/55 [00:27<00:22,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  56%|█████▋    | 31/55 [00:28<00:21,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  58%|█████▊    | 32/55 [00:29<00:21,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  60%|██████    | 33/55 [00:30<00:20,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  62%|██████▏   | 34/55 [00:31<00:19,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  64%|██████▎   | 35/55 [00:32<00:18,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  65%|██████▌   | 36/55 [00:32<00:17,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  67%|██████▋   | 37/55 [00:33<00:16,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  69%|██████▉   | 38/55 [00:34<00:15,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  71%|███████   | 39/55 [00:35<00:14,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  73%|███████▎  | 40/55 [00:36<00:13,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  75%|███████▍  | 41/55 [00:37<00:12,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  76%|███████▋  | 42/55 [00:38<00:11,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  78%|███████▊  | 43/55 [00:39<00:10,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  80%|████████  | 44/55 [00:40<00:10,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  82%|████████▏ | 45/55 [00:41<00:09,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  84%|████████▎ | 46/55 [00:42<00:08,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  85%|████████▌ | 47/55 [00:42<00:07,  1.09it/s]\u001b[A\n",
      "Epoch 6/10:  87%|████████▋ | 48/55 [00:43<00:06,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  89%|████████▉ | 49/55 [00:44<00:05,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  91%|█████████ | 50/55 [00:45<00:04,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  93%|█████████▎| 51/55 [00:46<00:03,  1.10it/s]\u001b[A\n",
      "Epoch 6/10:  95%|█████████▍| 52/55 [00:47<00:02,  1.11it/s]\u001b[A\n",
      "Epoch 6/10:  96%|█████████▋| 53/55 [00:48<00:01,  1.11it/s]\u001b[A\n",
      "Epoch 6/10:  98%|█████████▊| 54/55 [00:49<00:00,  1.11it/s]\u001b[A\n",
      "Epoch 6/10: 100%|██████████| 55/55 [00:50<00:00,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Training Loss: 2.2430\n",
      "Validation Loss: 2.2360\n",
      "Validation Accuracy: 19.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7/10:   2%|▏         | 1/55 [00:00<00:48,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:   4%|▎         | 2/55 [00:01<00:48,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:   5%|▌         | 3/55 [00:02<00:47,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:   7%|▋         | 4/55 [00:03<00:46,  1.11it/s]\u001b[A\n",
      "Epoch 7/10:   9%|▉         | 5/55 [00:04<00:45,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  11%|█         | 6/55 [00:05<00:44,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  13%|█▎        | 7/55 [00:06<00:43,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  15%|█▍        | 8/55 [00:07<00:42,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  16%|█▋        | 9/55 [00:08<00:41,  1.11it/s]\u001b[A\n",
      "Epoch 7/10:  18%|█▊        | 10/55 [00:09<00:40,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  20%|██        | 11/55 [00:09<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  22%|██▏       | 12/55 [00:10<00:38,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  24%|██▎       | 13/55 [00:11<00:37,  1.11it/s]\u001b[A\n",
      "Epoch 7/10:  25%|██▌       | 14/55 [00:12<00:37,  1.11it/s]\u001b[A\n",
      "Epoch 7/10:  27%|██▋       | 15/55 [00:13<00:36,  1.11it/s]\u001b[A\n",
      "Epoch 7/10:  29%|██▉       | 16/55 [00:14<00:35,  1.11it/s]\u001b[A\n",
      "Epoch 7/10:  31%|███       | 17/55 [00:15<00:34,  1.09it/s]\u001b[A\n",
      "Epoch 7/10:  33%|███▎      | 18/55 [00:16<00:33,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  35%|███▍      | 19/55 [00:17<00:32,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  36%|███▋      | 20/55 [00:18<00:31,  1.11it/s]\u001b[A\n",
      "Epoch 7/10:  38%|███▊      | 21/55 [00:19<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  40%|████      | 22/55 [00:19<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  42%|████▏     | 23/55 [00:20<00:29,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  44%|████▎     | 24/55 [00:21<00:28,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  45%|████▌     | 25/55 [00:22<00:27,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  47%|████▋     | 26/55 [00:23<00:26,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  49%|████▉     | 27/55 [00:24<00:25,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  51%|█████     | 28/55 [00:25<00:24,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  53%|█████▎    | 29/55 [00:26<00:23,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  55%|█████▍    | 30/55 [00:27<00:22,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  56%|█████▋    | 31/55 [00:28<00:21,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  58%|█████▊    | 32/55 [00:29<00:20,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  60%|██████    | 33/55 [00:29<00:20,  1.09it/s]\u001b[A\n",
      "Epoch 7/10:  62%|██████▏   | 34/55 [00:30<00:19,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  64%|██████▎   | 35/55 [00:31<00:18,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  65%|██████▌   | 36/55 [00:32<00:17,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  67%|██████▋   | 37/55 [00:33<00:16,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  69%|██████▉   | 38/55 [00:34<00:15,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  71%|███████   | 39/55 [00:35<00:14,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  73%|███████▎  | 40/55 [00:36<00:13,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  75%|███████▍  | 41/55 [00:37<00:12,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  76%|███████▋  | 42/55 [00:38<00:11,  1.11it/s]\u001b[A\n",
      "Epoch 7/10:  78%|███████▊  | 43/55 [00:39<00:10,  1.11it/s]\u001b[A\n",
      "Epoch 7/10:  80%|████████  | 44/55 [00:39<00:09,  1.11it/s]\u001b[A\n",
      "Epoch 7/10:  82%|████████▏ | 45/55 [00:40<00:09,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  84%|████████▎ | 46/55 [00:41<00:08,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  85%|████████▌ | 47/55 [00:42<00:07,  1.09it/s]\u001b[A\n",
      "Epoch 7/10:  87%|████████▋ | 48/55 [00:43<00:06,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  89%|████████▉ | 49/55 [00:44<00:05,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  91%|█████████ | 50/55 [00:45<00:04,  1.09it/s]\u001b[A\n",
      "Epoch 7/10:  93%|█████████▎| 51/55 [00:46<00:03,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  95%|█████████▍| 52/55 [00:47<00:02,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  96%|█████████▋| 53/55 [00:48<00:01,  1.10it/s]\u001b[A\n",
      "Epoch 7/10:  98%|█████████▊| 54/55 [00:49<00:00,  1.10it/s]\u001b[A\n",
      "Epoch 7/10: 100%|██████████| 55/55 [00:49<00:00,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "Training Loss: 2.2426\n",
      "Validation Loss: 2.2333\n",
      "Validation Accuracy: 19.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8/10:   2%|▏         | 1/55 [00:00<00:48,  1.12it/s]\u001b[A\n",
      "Epoch 8/10:   4%|▎         | 2/55 [00:01<00:47,  1.11it/s]\u001b[A\n",
      "Epoch 8/10:   5%|▌         | 3/55 [00:02<00:46,  1.11it/s]\u001b[A\n",
      "Epoch 8/10:   7%|▋         | 4/55 [00:03<00:46,  1.11it/s]\u001b[A\n",
      "Epoch 8/10:   9%|▉         | 5/55 [00:04<00:45,  1.11it/s]\u001b[A\n",
      "Epoch 8/10:  11%|█         | 6/55 [00:05<00:45,  1.08it/s]\u001b[A\n",
      "Epoch 8/10:  13%|█▎        | 7/55 [00:06<00:45,  1.06it/s]\u001b[A\n",
      "Epoch 8/10:  15%|█▍        | 8/55 [00:07<00:43,  1.07it/s]\u001b[A\n",
      "Epoch 8/10:  16%|█▋        | 9/55 [00:08<00:42,  1.07it/s]\u001b[A\n",
      "Epoch 8/10:  18%|█▊        | 10/55 [00:09<00:41,  1.08it/s]\u001b[A\n",
      "Epoch 8/10:  20%|██        | 11/55 [00:10<00:40,  1.09it/s]\u001b[A\n",
      "Epoch 8/10:  22%|██▏       | 12/55 [00:11<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  24%|██▎       | 13/55 [00:11<00:38,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  25%|██▌       | 14/55 [00:12<00:37,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  27%|██▋       | 15/55 [00:13<00:36,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  29%|██▉       | 16/55 [00:14<00:35,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  31%|███       | 17/55 [00:15<00:34,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  33%|███▎      | 18/55 [00:16<00:33,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  35%|███▍      | 19/55 [00:17<00:32,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  36%|███▋      | 20/55 [00:18<00:31,  1.09it/s]\u001b[A\n",
      "Epoch 8/10:  38%|███▊      | 21/55 [00:19<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  40%|████      | 22/55 [00:20<00:29,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  42%|████▏     | 23/55 [00:20<00:28,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  44%|████▎     | 24/55 [00:21<00:28,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  45%|████▌     | 25/55 [00:22<00:27,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  47%|████▋     | 26/55 [00:23<00:26,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  49%|████▉     | 27/55 [00:24<00:25,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  51%|█████     | 28/55 [00:25<00:24,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  53%|█████▎    | 29/55 [00:26<00:23,  1.09it/s]\u001b[A\n",
      "Epoch 8/10:  55%|█████▍    | 30/55 [00:27<00:22,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  56%|█████▋    | 31/55 [00:28<00:22,  1.09it/s]\u001b[A\n",
      "Epoch 8/10:  58%|█████▊    | 32/55 [00:29<00:21,  1.08it/s]\u001b[A\n",
      "Epoch 8/10:  60%|██████    | 33/55 [00:30<00:20,  1.09it/s]\u001b[A\n",
      "Epoch 8/10:  62%|██████▏   | 34/55 [00:31<00:19,  1.09it/s]\u001b[A\n",
      "Epoch 8/10:  64%|██████▎   | 35/55 [00:31<00:18,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  65%|██████▌   | 36/55 [00:32<00:17,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  67%|██████▋   | 37/55 [00:33<00:16,  1.11it/s]\u001b[A\n",
      "Epoch 8/10:  69%|██████▉   | 38/55 [00:34<00:15,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  71%|███████   | 39/55 [00:35<00:14,  1.11it/s]\u001b[A\n",
      "Epoch 8/10:  73%|███████▎  | 40/55 [00:36<00:13,  1.11it/s]\u001b[A\n",
      "Epoch 8/10:  75%|███████▍  | 41/55 [00:37<00:12,  1.09it/s]\u001b[A\n",
      "Epoch 8/10:  76%|███████▋  | 42/55 [00:38<00:11,  1.08it/s]\u001b[A\n",
      "Epoch 8/10:  78%|███████▊  | 43/55 [00:39<00:11,  1.09it/s]\u001b[A\n",
      "Epoch 8/10:  80%|████████  | 44/55 [00:40<00:10,  1.09it/s]\u001b[A\n",
      "Epoch 8/10:  82%|████████▏ | 45/55 [00:41<00:09,  1.09it/s]\u001b[A\n",
      "Epoch 8/10:  84%|████████▎ | 46/55 [00:41<00:08,  1.09it/s]\u001b[A\n",
      "Epoch 8/10:  85%|████████▌ | 47/55 [00:42<00:07,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  87%|████████▋ | 48/55 [00:43<00:06,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  89%|████████▉ | 49/55 [00:44<00:05,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  91%|█████████ | 50/55 [00:45<00:04,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  93%|█████████▎| 51/55 [00:46<00:03,  1.11it/s]\u001b[A\n",
      "Epoch 8/10:  95%|█████████▍| 52/55 [00:47<00:02,  1.11it/s]\u001b[A\n",
      "Epoch 8/10:  96%|█████████▋| 53/55 [00:48<00:01,  1.10it/s]\u001b[A\n",
      "Epoch 8/10:  98%|█████████▊| 54/55 [00:49<00:00,  1.10it/s]\u001b[A\n",
      "Epoch 8/10: 100%|██████████| 55/55 [00:50<00:00,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "Training Loss: 2.2404\n",
      "Validation Loss: 2.2347\n",
      "Validation Accuracy: 19.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9/10:   2%|▏         | 1/55 [00:00<00:48,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:   4%|▎         | 2/55 [00:01<00:48,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:   5%|▌         | 3/55 [00:02<00:47,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:   7%|▋         | 4/55 [00:03<00:46,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:   9%|▉         | 5/55 [00:04<00:45,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  11%|█         | 6/55 [00:05<00:44,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  13%|█▎        | 7/55 [00:06<00:43,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:  15%|█▍        | 8/55 [00:07<00:42,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:  16%|█▋        | 9/55 [00:08<00:41,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:  18%|█▊        | 10/55 [00:09<00:40,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  20%|██        | 11/55 [00:09<00:39,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:  22%|██▏       | 12/55 [00:10<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  24%|██▎       | 13/55 [00:11<00:38,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  25%|██▌       | 14/55 [00:12<00:37,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  27%|██▋       | 15/55 [00:13<00:37,  1.07it/s]\u001b[A\n",
      "Epoch 9/10:  29%|██▉       | 16/55 [00:14<00:36,  1.08it/s]\u001b[A\n",
      "Epoch 9/10:  31%|███       | 17/55 [00:15<00:34,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  33%|███▎      | 18/55 [00:16<00:33,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  35%|███▍      | 19/55 [00:17<00:32,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  36%|███▋      | 20/55 [00:18<00:32,  1.08it/s]\u001b[A\n",
      "Epoch 9/10:  38%|███▊      | 21/55 [00:19<00:31,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  40%|████      | 22/55 [00:20<00:30,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  42%|████▏     | 23/55 [00:21<00:29,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  44%|████▎     | 24/55 [00:21<00:28,  1.08it/s]\u001b[A\n",
      "Epoch 9/10:  45%|████▌     | 25/55 [00:22<00:27,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  47%|████▋     | 26/55 [00:23<00:26,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  49%|████▉     | 27/55 [00:24<00:25,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  51%|█████     | 28/55 [00:25<00:24,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  53%|█████▎    | 29/55 [00:26<00:23,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  55%|█████▍    | 30/55 [00:27<00:22,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  56%|█████▋    | 31/55 [00:28<00:21,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:  58%|█████▊    | 32/55 [00:29<00:20,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:  60%|██████    | 33/55 [00:30<00:19,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:  62%|██████▏   | 34/55 [00:30<00:18,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:  64%|██████▎   | 35/55 [00:31<00:18,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  65%|██████▌   | 36/55 [00:32<00:17,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  67%|██████▋   | 37/55 [00:33<00:16,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  69%|██████▉   | 38/55 [00:34<00:15,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  71%|███████   | 39/55 [00:35<00:14,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  73%|███████▎  | 40/55 [00:36<00:13,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  75%|███████▍  | 41/55 [00:37<00:12,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  76%|███████▋  | 42/55 [00:38<00:11,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:  78%|███████▊  | 43/55 [00:39<00:10,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  80%|████████  | 44/55 [00:40<00:09,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:  82%|████████▏ | 45/55 [00:40<00:09,  1.11it/s]\u001b[A\n",
      "Epoch 9/10:  84%|████████▎ | 46/55 [00:41<00:08,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  85%|████████▌ | 47/55 [00:42<00:07,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  87%|████████▋ | 48/55 [00:43<00:06,  1.10it/s]\u001b[A\n",
      "Epoch 9/10:  89%|████████▉ | 49/55 [00:44<00:05,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  91%|█████████ | 50/55 [00:45<00:04,  1.08it/s]\u001b[A\n",
      "Epoch 9/10:  93%|█████████▎| 51/55 [00:46<00:03,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  95%|█████████▍| 52/55 [00:47<00:02,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  96%|█████████▋| 53/55 [00:48<00:01,  1.09it/s]\u001b[A\n",
      "Epoch 9/10:  98%|█████████▊| 54/55 [00:49<00:00,  1.10it/s]\u001b[A\n",
      "Epoch 9/10: 100%|██████████| 55/55 [00:50<00:00,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "Training Loss: 2.2401\n",
      "Validation Loss: 2.2352\n",
      "Validation Accuracy: 19.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10/10:   2%|▏         | 1/55 [00:00<00:48,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:   4%|▎         | 2/55 [00:01<00:47,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:   5%|▌         | 3/55 [00:02<00:47,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:   7%|▋         | 4/55 [00:03<00:46,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:   9%|▉         | 5/55 [00:04<00:45,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:  11%|█         | 6/55 [00:05<00:44,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  13%|█▎        | 7/55 [00:06<00:43,  1.09it/s]\u001b[A\n",
      "Epoch 10/10:  15%|█▍        | 8/55 [00:07<00:42,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  16%|█▋        | 9/55 [00:08<00:41,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  18%|█▊        | 10/55 [00:09<00:41,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  20%|██        | 11/55 [00:10<00:40,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  22%|██▏       | 12/55 [00:10<00:39,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  24%|██▎       | 13/55 [00:11<00:38,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  25%|██▌       | 14/55 [00:12<00:37,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  27%|██▋       | 15/55 [00:13<00:36,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  29%|██▉       | 16/55 [00:14<00:35,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:  31%|███       | 17/55 [00:15<00:34,  1.09it/s]\u001b[A\n",
      "Epoch 10/10:  33%|███▎      | 18/55 [00:16<00:33,  1.09it/s]\u001b[A\n",
      "Epoch 10/10:  35%|███▍      | 19/55 [00:17<00:32,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  36%|███▋      | 20/55 [00:18<00:31,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  38%|███▊      | 21/55 [00:19<00:30,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  40%|████      | 22/55 [00:20<00:29,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  42%|████▏     | 23/55 [00:20<00:29,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  44%|████▎     | 24/55 [00:21<00:28,  1.08it/s]\u001b[A\n",
      "Epoch 10/10:  45%|████▌     | 25/55 [00:22<00:27,  1.09it/s]\u001b[A\n",
      "Epoch 10/10:  47%|████▋     | 26/55 [00:23<00:26,  1.09it/s]\u001b[A\n",
      "Epoch 10/10:  49%|████▉     | 27/55 [00:24<00:25,  1.09it/s]\u001b[A\n",
      "Epoch 10/10:  51%|█████     | 28/55 [00:25<00:24,  1.09it/s]\u001b[A\n",
      "Epoch 10/10:  53%|█████▎    | 29/55 [00:26<00:23,  1.09it/s]\u001b[A\n",
      "Epoch 10/10:  55%|█████▍    | 30/55 [00:27<00:22,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  56%|█████▋    | 31/55 [00:28<00:21,  1.09it/s]\u001b[A\n",
      "Epoch 10/10:  58%|█████▊    | 32/55 [00:29<00:20,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  60%|██████    | 33/55 [00:30<00:19,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:  62%|██████▏   | 34/55 [00:30<00:18,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:  64%|██████▎   | 35/55 [00:31<00:18,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:  65%|██████▌   | 36/55 [00:32<00:17,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:  67%|██████▋   | 37/55 [00:33<00:16,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:  69%|██████▉   | 38/55 [00:34<00:15,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:  71%|███████   | 39/55 [00:35<00:14,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  73%|███████▎  | 40/55 [00:36<00:13,  1.09it/s]\u001b[A\n",
      "Epoch 10/10:  75%|███████▍  | 41/55 [00:37<00:12,  1.09it/s]\u001b[A\n",
      "Epoch 10/10:  76%|███████▋  | 42/55 [00:38<00:11,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  78%|███████▊  | 43/55 [00:39<00:10,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  80%|████████  | 44/55 [00:40<00:09,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  82%|████████▏ | 45/55 [00:40<00:09,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  84%|████████▎ | 46/55 [00:41<00:08,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:  85%|████████▌ | 47/55 [00:42<00:07,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:  87%|████████▋ | 48/55 [00:43<00:06,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  89%|████████▉ | 49/55 [00:44<00:05,  1.11it/s]\u001b[A\n",
      "Epoch 10/10:  91%|█████████ | 50/55 [00:45<00:04,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  93%|█████████▎| 51/55 [00:46<00:03,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  95%|█████████▍| 52/55 [00:47<00:02,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  96%|█████████▋| 53/55 [00:48<00:01,  1.10it/s]\u001b[A\n",
      "Epoch 10/10:  98%|█████████▊| 54/55 [00:49<00:00,  1.10it/s]\u001b[A\n",
      "Epoch 10/10: 100%|██████████| 55/55 [00:49<00:00,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "Training Loss: 2.2394\n",
      "Validation Loss: 2.2345\n",
      "Validation Accuracy: 19.22%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ListOpsDataset(Dataset):\n",
    "    def __init__(self, file_path, max_length=4096):\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        for text, label in zip(df['Source'], df['Target']):\n",
    "            if len(text) < 10000:\n",
    "                self.texts.append(text)\n",
    "                self.labels.append(label)\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.vocab = {\n",
    "            'PAD': 0, '[': 1, ']': 2, 'SM': 3, 'MAX': 4,\n",
    "            'MIN': 5, 'MED': 6, '0': 7, '1': 8, '2': 9,\n",
    "            '3': 10, '4': 11, '5': 12, '6': 13, '7': 14,\n",
    "            '8': 15, '9': 16, '(': 17, ')': 18, ' ': 19, '\\t': 20\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = [self.vocab[char] for char in text if char in self.vocab]\n",
    "        \n",
    "        attention_mask = [1] * min(len(tokens), self.max_length)\n",
    "        tokens = tokens[:self.max_length]\n",
    "        \n",
    "        if len(tokens) < self.max_length:\n",
    "            padding_length = self.max_length - len(tokens)\n",
    "            tokens = tokens + [self.vocab['PAD']] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length\n",
    "            \n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class RNNAttention(nn.Module):\n",
    "    def __init__(self, vocab_size=21, embedding_dim=128, hidden_size=256, num_layers=2, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def attention_net(self, lstm_output, mask):\n",
    "        attention_weights = self.attention(lstm_output).squeeze(-1)\n",
    "        attention_weights = attention_weights.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), lstm_output).squeeze(1)\n",
    "        return context\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.dropout(self.embedding(input_ids))\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        context = self.attention_net(output, attention_mask)\n",
    "        output = self.fc(context)\n",
    "        return output\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def train_model(train_path, val_path, batch_size=32, num_epochs=10, learning_rate=1e-3):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    train_dataset = ListOpsDataset(train_path)\n",
    "    val_dataset = ListOpsDataset(val_path)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    model = RNNAttention().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        print(f'Validation Accuracy: {val_acc:.2f}%')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'New best model saved with validation accuracy: {val_acc:.2f}%')\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        train_path='/kaggle/input/lra-listops/basic_test.tsv',\n",
    "        val_path='/kaggle/input/validation/basic_val.tsv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigbird model with preprocessing and tokenizer using 10 % of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:37:33.449716Z",
     "iopub.status.busy": "2024-12-04T18:37:33.449333Z",
     "iopub.status.idle": "2024-12-04T19:47:15.010184Z",
     "shell.execute_reply": "2024-12-04T19:47:15.009308Z",
     "shell.execute_reply.started": "2024-12-04T18:37:33.449684Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 9600 samples\n",
      "Validating on 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 960/960 [10:38<00:00,  1.50it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train - Loss: 2.2711, Accuracy: 0.1654, F1: 0.0470\n",
      "Val   - Loss: 2.2344, Accuracy: 0.1750, F1: 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 960/960 [10:38<00:00,  1.50it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train - Loss: 2.2604, Accuracy: 0.1627, F1: 0.0455\n",
      "Val   - Loss: 2.2252, Accuracy: 0.1850, F1: 0.0578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 960/960 [10:37<00:00,  1.51it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train - Loss: 2.2585, Accuracy: 0.1654, F1: 0.0470\n",
      "Val   - Loss: 2.2361, Accuracy: 0.1750, F1: 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 960/960 [10:38<00:00,  1.50it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train - Loss: 2.2570, Accuracy: 0.1654, F1: 0.0470\n",
      "Val   - Loss: 2.2350, Accuracy: 0.1750, F1: 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 960/960 [10:37<00:00,  1.51it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Train - Loss: 2.2558, Accuracy: 0.1654, F1: 0.0470\n",
      "Val   - Loss: 2.2317, Accuracy: 0.1750, F1: 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BigBirdConfig, BigBirdModel, PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, processors\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def load_tokenizer():\n",
    "    # Create vocabulary\n",
    "    vocab = {\n",
    "        \"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4,\n",
    "        \"(\": 5, \")\": 6, \"[\": 7, \"]\": 8, \"8\": 9, \"7\": 10, \"9\": 11,\n",
    "        \"5\": 12, \"4\": 13, \"6\": 14, \"3\": 15, \"1\": 16, \"0\": 17, \"2\": 18,\n",
    "        \"MIN\": 19, \"MED\": 20, \"MAX\": 21, \"SM\": 22\n",
    "    }\n",
    "    \n",
    "    # Create a WordLevel tokenizer\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab, unk_token=\"[UNK]\"))\n",
    "    \n",
    "    # Add whitespace pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    \n",
    "    # Create the wrapped tokenizer\n",
    "    wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        mask_token=\"[MASK]\"\n",
    "    )\n",
    "    \n",
    "    return wrapped_tokenizer\n",
    "\n",
    "class ListOpsDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=8192, use_subset=True):\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        \n",
    "        # Use only 10% of the data if use_subset is True\n",
    "        if use_subset:\n",
    "            df = df.sample(frac=0.1, random_state=42)\n",
    "        \n",
    "        self.texts = df['Source'].tolist()\n",
    "        self.labels = df['Target'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'][0],\n",
    "            'attention_mask': encoding['attention_mask'][0],\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class BigBirdListOps(nn.Module):\n",
    "    def __init__(self, vocab_size=23, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.config = BigBirdConfig(\n",
    "            hidden_size=8,\n",
    "            num_attention_heads=4,\n",
    "            intermediate_size=512,\n",
    "            num_hidden_layers=2,\n",
    "            vocab_size=vocab_size,\n",
    "            max_position_embeddings=8192,\n",
    "            attention_type=\"block_sparse\",\n",
    "            block_size=64,\n",
    "            num_random_blocks=2\n",
    "        )\n",
    "        self.bigbird = BigBirdModel(self.config)\n",
    "        self.classifier = nn.Linear(8, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bigbird(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "        return logits\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def train_model(train_path, val_path, batch_size=10, num_epochs=5, learning_rate=1e-4):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = load_tokenizer()\n",
    "    \n",
    "    # Create datasets with 10% of the data\n",
    "    train_dataset = ListOpsDataset(train_path, tokenizer, use_subset=True)\n",
    "    val_dataset = ListOpsDataset(val_path, tokenizer, use_subset=True)\n",
    "    \n",
    "    print(f\"Training on {len(train_dataset)} samples\")\n",
    "    print(f\"Validating on {len(val_dataset)} samples\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = BigBirdListOps(vocab_size=23).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_metrics = evaluate(model, train_loader, criterion, device)\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Train - Loss: {train_metrics[\"loss\"]:.4f}, Accuracy: {train_metrics[\"accuracy\"]:.4f}, F1: {train_metrics[\"f1\"]:.4f}')\n",
    "        print(f'Val   - Loss: {val_metrics[\"loss\"]:.4f}, Accuracy: {val_metrics[\"accuracy\"]:.4f}, F1: {val_metrics[\"f1\"]:.4f}')\n",
    "        \n",
    "        torch.save(model.state_dict(), f'bigbird_listops_epoch_{epoch+1}.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        train_path='/kaggle/input/lra-listops/basic_train.tsv',\n",
    "        val_path='/kaggle/input/validation/basic_val.tsv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigbird Model using test and train datasets with sequences of 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T22:13:37.154641Z",
     "iopub.status.busy": "2024-12-05T22:13:37.154083Z",
     "iopub.status.idle": "2024-12-05T22:29:12.983687Z",
     "shell.execute_reply": "2024-12-05T22:29:12.982813Z",
     "shell.execute_reply.started": "2024-12-05T22:13:37.154610Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 9000 samples\n",
      "Validating on 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 900/900 [01:07<00:00, 13.37it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train - Loss: 2.3058, Accuracy: 0.1127, F1: 0.0355\n",
      "Val   - Loss: 2.2934, Accuracy: 0.1160, F1: 0.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 900/900 [01:07<00:00, 13.32it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train - Loss: 2.2483, Accuracy: 0.1832, F1: 0.1031\n",
      "Val   - Loss: 2.2694, Accuracy: 0.1630, F1: 0.0899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 900/900 [01:07<00:00, 13.30it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train - Loss: 2.2366, Accuracy: 0.1788, F1: 0.1241\n",
      "Val   - Loss: 2.2625, Accuracy: 0.1680, F1: 0.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 900/900 [01:07<00:00, 13.32it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train - Loss: 2.2229, Accuracy: 0.1931, F1: 0.1107\n",
      "Val   - Loss: 2.2496, Accuracy: 0.1770, F1: 0.1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 900/900 [01:07<00:00, 13.31it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Train - Loss: 2.2231, Accuracy: 0.1900, F1: 0.1229\n",
      "Val   - Loss: 2.2530, Accuracy: 0.1740, F1: 0.1087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 900/900 [01:07<00:00, 13.31it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Train - Loss: 2.2055, Accuracy: 0.2063, F1: 0.1285\n",
      "Val   - Loss: 2.2540, Accuracy: 0.1850, F1: 0.1177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 900/900 [01:07<00:00, 13.28it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "Train - Loss: 2.2019, Accuracy: 0.2089, F1: 0.1388\n",
      "Val   - Loss: 2.2615, Accuracy: 0.1820, F1: 0.1211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 900/900 [01:07<00:00, 13.27it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "Train - Loss: 2.1898, Accuracy: 0.2187, F1: 0.1395\n",
      "Val   - Loss: 2.2481, Accuracy: 0.1840, F1: 0.1171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 900/900 [01:07<00:00, 13.27it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "Train - Loss: 2.1780, Accuracy: 0.2219, F1: 0.1434\n",
      "Val   - Loss: 2.2489, Accuracy: 0.1910, F1: 0.1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 900/900 [01:07<00:00, 13.26it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "Train - Loss: 2.1707, Accuracy: 0.2211, F1: 0.1464\n",
      "Val   - Loss: 2.2458, Accuracy: 0.1850, F1: 0.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BigBirdConfig, BigBirdModel, PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, processors\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def load_tokenizer():\n",
    "    # Create vocabulary\n",
    "    vocab = {\n",
    "        \"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4,\n",
    "        \"(\": 5, \")\": 6, \"[\": 7, \"]\": 8, \"8\": 9, \"7\": 10, \"9\": 11,\n",
    "        \"5\": 12, \"4\": 13, \"6\": 14, \"3\": 15, \"1\": 16, \"0\": 17, \"2\": 18,\n",
    "        \"MIN\": 19, \"MED\": 20, \"MAX\": 21, \"SM\": 22\n",
    "    }\n",
    "    \n",
    "    # Create a WordLevel tokenizer\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab, unk_token=\"[UNK]\"))\n",
    "    \n",
    "    # Add whitespace pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    \n",
    "    # Create the wrapped tokenizer\n",
    "    wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        mask_token=\"[MASK]\"\n",
    "    )\n",
    "    \n",
    "    return wrapped_tokenizer\n",
    "\n",
    "class ListOpsDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=1024, use_subset=True):\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        \n",
    "        # Use only 10% of the data if use_subset is True\n",
    "        if use_subset:\n",
    "            df = df.sample(frac=0.1, random_state=42)\n",
    "        \n",
    "        self.texts = df['Source'].tolist()\n",
    "        self.labels = df['Target'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'][0],\n",
    "            'attention_mask': encoding['attention_mask'][0],\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class BigBirdListOps(nn.Module):\n",
    "    def __init__(self, vocab_size=23, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.config = BigBirdConfig(\n",
    "            hidden_size=8,\n",
    "            num_attention_heads=4,\n",
    "            intermediate_size=512,\n",
    "            num_hidden_layers=2,\n",
    "            vocab_size=vocab_size,\n",
    "            max_position_embeddings=1024,\n",
    "            attention_type=\"block_sparse\",\n",
    "            block_size=64,\n",
    "            num_random_blocks=2\n",
    "        )\n",
    "        self.bigbird = BigBirdModel(self.config)\n",
    "        self.classifier = nn.Linear(8, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bigbird(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "        return logits\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def train_model(train_path, val_path, batch_size=10, num_epochs=10, learning_rate=1e-3):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = load_tokenizer()\n",
    "    \n",
    "    # Create datasets with 10% of the data\n",
    "    train_dataset = ListOpsDataset(train_path, tokenizer, use_subset=True)\n",
    "    val_dataset = ListOpsDataset(val_path, tokenizer, use_subset=True)\n",
    "    \n",
    "    print(f\"Training on {len(train_dataset)} samples\")\n",
    "    print(f\"Validating on {len(val_dataset)} samples\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = BigBirdListOps(vocab_size=23).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_metrics = evaluate(model, train_loader, criterion, device)\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Train - Loss: {train_metrics[\"loss\"]:.4f}, Accuracy: {train_metrics[\"accuracy\"]:.4f}, F1: {train_metrics[\"f1\"]:.4f}')\n",
    "        print(f'Val   - Loss: {val_metrics[\"loss\"]:.4f}, Accuracy: {val_metrics[\"accuracy\"]:.4f}, F1: {val_metrics[\"f1\"]:.4f}')\n",
    "        \n",
    "        torch.save(model.state_dict(), f'bigbird_listops_epoch_{epoch+1}.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        train_path='/kaggle/input/listops/train_d20s.tsv',\n",
    "        val_path='/kaggle/input/listops/test_d20s.tsv'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6213643,
     "sourceId": 10079404,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6217911,
     "sourceId": 10085161,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6239887,
     "sourceId": 10113799,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
