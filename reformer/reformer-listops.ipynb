{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10079404,"sourceType":"datasetVersion","datasetId":6213643},{"sourceId":10113626,"sourceType":"datasetVersion","datasetId":6239759},{"sourceId":10121160,"sourceType":"datasetVersion","datasetId":6245231}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Install the required version of transformers\n!pip install -U transformers==4.40.2\n\n# Import necessary libraries\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nimport os\nfrom transformers import ReformerConfig, ReformerForSequenceClassification, AutoTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport torch.nn as nn\n","metadata":{"id":"bnXKpCNPf9Sk","outputId":"ecc5d481-aa5b-4ea6-a956-2e49ddca5e12","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:26:53.056304Z","iopub.execute_input":"2024-12-06T21:26:53.056935Z","iopub.status.idle":"2024-12-06T21:27:01.477095Z","shell.execute_reply.started":"2024-12-06T21:26:53.056895Z","shell.execute_reply":"2024-12-06T21:27:01.475833Z"},"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing Data","metadata":{}},{"cell_type":"code","source":"# Define the file paths\ntrain_file = '/kaggle/input/depth-20/train_d20s.tsv'\ntest_file = '/kaggle/input/depth-20/test_d20s.tsv'\n\n# Load the data\ntrain_df = pd.read_csv(train_file, sep='\\t', header=0)\ntest_df = pd.read_csv(test_file, sep='\\t', header=0)\n# Preview the data\nprint(train_df.head())\n\n# Load the data\ntrain_df = pd.read_csv(train_file, sep='\\t', header=0)\ntest_df = pd.read_csv(test_file, sep='\\t', header=0)\n# Preview the data\nprint(train_df.head())\nprint(train_df.columns)\n","metadata":{"id":"s2NiNMIVf9Sp","outputId":"6bf37d1b-9cc5-44a8-b2bd-52bf9fce1cac","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:01.479214Z","iopub.execute_input":"2024-12-06T21:27:01.479556Z","iopub.status.idle":"2024-12-06T21:27:02.163879Z","shell.execute_reply.started":"2024-12-06T21:27:01.479523Z","shell.execute_reply":"2024-12-06T21:27:02.162927Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Basic data cleaning && exploration","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    return ''.join(str(text).split())\n\n# Apply the cleaning function to the 'Source' column\n#train_df['Source'] = train_df['Source'].apply(clean_text)\n#test_df['Source'] = test_df['Source'].apply(clean_text)\n","metadata":{"id":"RoggwnV9f9Sq","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:02.165152Z","iopub.execute_input":"2024-12-06T21:27:02.165429Z","iopub.status.idle":"2024-12-06T21:27:02.170060Z","shell.execute_reply.started":"2024-12-06T21:27:02.165404Z","shell.execute_reply":"2024-12-06T21:27:02.169054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Remove any possible header rows included as data\ntrain_df = train_df[train_df['Target'] != 'Target']\ntest_df = test_df[test_df['Target'] != 'Target']\n\n# Convert labels to integers\ntrain_df['Target'] = train_df['Target'].astype(int)\ntest_df['Target'] = test_df['Target'].astype(int)\nprint(train_df.head())\nprint(\"---------------\")\nfor i in range(8) : \n    print(len(train_df['Source'][i]))","metadata":{"id":"rEG3i6q-f9Sr","outputId":"10390f55-b653-41ff-fed5-466c7a71c427","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:02.171803Z","iopub.execute_input":"2024-12-06T21:27:02.172062Z","iopub.status.idle":"2024-12-06T21:27:02.189614Z","shell.execute_reply.started":"2024-12-06T21:27:02.172038Z","shell.execute_reply":"2024-12-06T21:27:02.188717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute the lengths of the original sequences\nseqLengths= train_df['Source'].apply(lambda x: len(x))\n\n# Describe the sequence lengths\nprint(\"Training data sequence lengths:\")\nprint(seqLengths.describe())\n\nprint(seqLengths.head())\n\n\n\n","metadata":{"id":"_lRw41IPf9Ss","outputId":"a913a18a-8930-4ce7-99b3-b92b6884e116","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:02.190734Z","iopub.execute_input":"2024-12-06T21:27:02.191086Z","iopub.status.idle":"2024-12-06T21:27:02.241470Z","shell.execute_reply.started":"2024-12-06T21:27:02.191048Z","shell.execute_reply":"2024-12-06T21:27:02.240589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Filter the training and test datasets based on the condition\ntrain_df = train_df[train_df['Source'].apply(lambda x: len(x)) < 1024]\ntest_df = test_df[test_df['Source'].apply(lambda x: len(x)) < 1024]\n\n# # Randomly sample a percentage of the filtered train data\n# train_df = train_df.sample(frac=1, random_state=42)  # Set random_state for reproducibility\n\n# Check the filtered dataframe\nseqLengths = train_df['Source'].apply(lambda x: len(x))\n\n# Describe the sequence lengths\nprint(\"Training data sequence lengths:\")\nprint(seqLengths.describe())\n\nprint(seqLengths.head())\n","metadata":{"id":"l3GY0_E7f9St","outputId":"b2dbf190-0076-43c7-e477-55730679624d","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:02.242763Z","iopub.execute_input":"2024-12-06T21:27:02.243060Z","iopub.status.idle":"2024-12-06T21:27:02.338433Z","shell.execute_reply.started":"2024-12-06T21:27:02.243031Z","shell.execute_reply":"2024-12-06T21:27:02.337424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the sequences to a text file for tokenizer training\nwith open(\"listops_sequences.txt\", \"w\") as f:\n    for sequence in train_df[\"Source\"]:\n        f.write(sequence + \"\\n\")\n","metadata":{"id":"epaeuELUf9Su","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:02.339675Z","iopub.execute_input":"2024-12-06T21:27:02.339946Z","iopub.status.idle":"2024-12-06T21:27:02.418871Z","shell.execute_reply.started":"2024-12-06T21:27:02.339920Z","shell.execute_reply":"2024-12-06T21:27:02.417924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# Initialize a WordLevel tokenizer\ntokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n\n# Set the pre-tokenization strategy\ntokenizer.pre_tokenizer = Whitespace()\n\n# Prepare a trainer with special tokens\ntrainer = WordLevelTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n\n# Train the tokenizer on your text file\ntokenizer.train([\"listops_sequences.txt\"], trainer)\n\n# Save the tokenizer\ntokenizer.save(\"custom_tokenizer.json\")\n","metadata":{"id":"4sjbfMHNf9Sv","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:02.419979Z","iopub.execute_input":"2024-12-06T21:27:02.420336Z","iopub.status.idle":"2024-12-06T21:27:04.063916Z","shell.execute_reply.started":"2024-12-06T21:27:02.420300Z","shell.execute_reply":"2024-12-06T21:27:04.062949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\n\n# Load the custom tokenizer\ntokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nprint(tokenizer)","metadata":{"id":"-zK6YG-Uf9Sv","outputId":"45587c65-36f7-47ed-85f3-ad5416e148f6","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:04.065194Z","iopub.execute_input":"2024-12-06T21:27:04.065474Z","iopub.status.idle":"2024-12-06T21:27:04.071136Z","shell.execute_reply.started":"2024-12-06T21:27:04.065427Z","shell.execute_reply":"2024-12-06T21:27:04.070268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LRADataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels.astype(int)  # Ensure labels are integers\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item]\n\n        # Tokenize and encode the text\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"id":"ZdJ7y6hcf9Sw","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:04.073523Z","iopub.execute_input":"2024-12-06T21:27:04.073879Z","iopub.status.idle":"2024-12-06T21:27:04.083917Z","shell.execute_reply.started":"2024-12-06T21:27:04.073854Z","shell.execute_reply":"2024-12-06T21:27:04.083034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    dataset = LRADataset(\n        texts=df['Source'].to_numpy(),\n        labels=df['Target'].to_numpy(),\n        tokenizer=tokenizer,\n        max_len=max_len\n    )\n\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=10\n    )\n\n# Parameters\nMAX_LEN = 1024\nBATCH_SIZE = 64\n\n# Create data loaders\ntrain_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n","metadata":{"id":"pRX285REf9Sw","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:04.084880Z","iopub.execute_input":"2024-12-06T21:27:04.085144Z","iopub.status.idle":"2024-12-06T21:27:04.097382Z","shell.execute_reply.started":"2024-12-06T21:27:04.085119Z","shell.execute_reply":"2024-12-06T21:27:04.096498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_labels = train_df[\"Source\"].nunique()\nprint(num_labels)\nprint(tokenizer.vocab_size)\n","metadata":{"id":"i2eA5Lhgf9Sx","outputId":"d10260f8-7d5d-4a7c-c8ad-8da0fd1f8654","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:04.098392Z","iopub.execute_input":"2024-12-06T21:27:04.098669Z","iopub.status.idle":"2024-12-06T21:27:04.165079Z","shell.execute_reply.started":"2024-12-06T21:27:04.098647Z","shell.execute_reply":"2024-12-06T21:27:04.164203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = ReformerConfig(\n    vocab_size=23,                    # ListOps vocab size\n    hidden_size=64,                  # Hidden state dimensionality\n    num_hidden_layers=6,              # Number of layers\n    num_attention_heads=8,            # 8 attention heads\n    attention_head_size=32,           # Size of each attention head\n    axial_pos_embds=True,             # Use axial positional embeddings\n    axial_pos_embds_dim=[32, 32],   # Axial dimensions (sum = hidden_size)\n    axial_pos_shape=[32, 32],         # Axial shape (product = sequence length)\n    attn_layers=[\"local\", \"lsh\", \"local\", \"lsh\", \"local\", \"lsh\"],  # Match num_hidden_layers\n    feed_forward_size=512,            # Large feed-forward size\n    lsh_num_chunks_before=2,          # More chunked look-backs for LSH\n    lsh_num_chunks_after=1,           # Chunked look-ahead for LSH\n    lsh_attention_probs_dropout_prob=0.1,  # Dropout for LSH attention\n    local_chunk_length=64,            # Local attention chunk size\n    local_attention_probs_dropout_prob=0.1,  # Dropout for local attention\n    hidden_dropout_prob=0.1,          # Dropout for fully connected layers\n    max_position_embeddings=1024,     # Maximum sequence length\n    num_labels=10,                    # Number of output labels\n)\n\n# Initialize the model\nmodel = ReformerForSequenceClassification(config)\n\n# Display the configuration to verify\nprint(model.config)\n","metadata":{"id":"O9vpnzKqf9Sx","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:04.166117Z","iopub.execute_input":"2024-12-06T21:27:04.166379Z","iopub.status.idle":"2024-12-06T21:27:04.186311Z","shell.execute_reply.started":"2024-12-06T21:27:04.166354Z","shell.execute_reply":"2024-12-06T21:27:04.185358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model)  # Wrap the model for multiple GPUs\n\nmodel = model.to(device)\n\n# Optimizer and scheduler\nEPOCHS = 5\noptimizer = AdamW(model.parameters(), lr=1e-3)\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=total_steps // 10,\n    num_training_steps=total_steps\n)\n\n# Loss function\nloss_fn = torch.nn.CrossEntropyLoss().to(device)\n","metadata":{"id":"R-EhrN8Qf9Sy","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:04.187316Z","iopub.execute_input":"2024-12-06T21:27:04.187591Z","iopub.status.idle":"2024-12-06T21:27:04.200068Z","shell.execute_reply.started":"2024-12-06T21:27:04.187566Z","shell.execute_reply":"2024-12-06T21:27:04.199300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(\n    model,\n    data_loader,\n    loss_fn,\n    optimizer,\n    device,\n    scheduler\n):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n\n    for batch in data_loader:\n        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n        labels = batch[\"labels\"].to(device, non_blocking=True)\n\n        outputs = model(\n            input_ids=input_ids.cuda(),\n            attention_mask=attention_mask.cuda(),\n            labels=labels.cuda()\n        )\n        loss = outputs.loss\n        logits = outputs.logits\n        _, preds = torch.max(logits, dim=1)\n\n        correct_predictions += torch.sum(preds == labels)\n        losses.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n\ndef eval_model(\n    model,\n    data_loader,\n    loss_fn,\n    device\n):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n            labels = batch[\"labels\"].to(device, non_blocking=True)\n\n            outputs = model(\n                input_ids=input_ids.cuda(),\n                attention_mask=attention_mask.cuda(),\n                labels=labels.cuda()\n            )\n            loss = outputs.loss\n            logits = outputs.logits\n            _, preds = torch.max(logits, dim=1)\n\n            correct_predictions += torch.sum(preds == labels)\n            losses.append(loss.item())\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n","metadata":{"id":"lBgjLgLbf9Sz","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:04.201025Z","iopub.execute_input":"2024-12-06T21:27:04.201254Z","iopub.status.idle":"2024-12-06T21:27:04.212060Z","shell.execute_reply.started":"2024-12-06T21:27:04.201231Z","shell.execute_reply":"2024-12-06T21:27:04.211266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = {\n    'train_acc': [],\n    'train_loss': [],\n    'val_acc': [],\n    'val_loss': []\n}\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n\n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler\n    )\n\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n\n    val_acc, val_loss = eval_model(\n        model,\n        test_data_loader,\n        loss_fn,\n        device\n    )\n\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n\n    history['train_acc'].append(train_acc.cpu().numpy())\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc.cpu().numpy())\n    history['val_loss'].append(val_loss)\n","metadata":{"id":"6j57p2O3f9Sz","outputId":"f49cd757-75b3-467c-d725-00059ccfed91","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:27:04.212875Z","iopub.execute_input":"2024-12-06T21:27:04.213130Z","iopub.status.idle":"2024-12-06T21:38:27.547993Z","shell.execute_reply.started":"2024-12-06T21:27:04.213107Z","shell.execute_reply":"2024-12-06T21:38:27.546514Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the trained model","metadata":{}},{"cell_type":"code","source":"# Specify the directory to save the model\noutput_dir = './my_model_listops_reduced_best_scheduled/'\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Save the trained model and tokenizer\nmodel_to_save = model.module if hasattr(model, 'module') else model\nmodel_to_save.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model saved to {output_dir}\")\n","metadata":{"id":"o1Yjmz2_f9S0","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:38:27.549237Z","iopub.status.idle":"2024-12-06T21:38:27.549721Z","shell.execute_reply.started":"2024-12-06T21:38:27.549485Z","shell.execute_reply":"2024-12-06T21:38:27.549510Z"}},"outputs":[],"execution_count":null}]}