{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10113626,"sourceType":"datasetVersion","datasetId":6239759}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the required libraries\n!pip install -U transformers==4.40.2 pytorch-lightning torchmetrics\n\n# Import necessary libraries\nimport torch\nfrom transformers import MegaConfig, MegaForSequenceClassification, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nimport os\nimport pytorch_lightning as pl\nfrom torchmetrics import Accuracy\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:55:15.733275Z","iopub.execute_input":"2024-12-06T19:55:15.733942Z","iopub.status.idle":"2024-12-06T19:55:42.348944Z","shell.execute_reply.started":"2024-12-06T19:55:15.733908Z","shell.execute_reply":"2024-12-06T19:55:42.347975Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.40.2\n  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (2.32.3)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (4.66.4)\nRequirement already satisfied: torch>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (2.4.0)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.0)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.12.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (0.11.9)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (70.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.40.2) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.0->pytorch-lightning) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (2024.6.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\nDownloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.3\n    Uninstalling tokenizers-0.20.3:\n      Successfully uninstalled tokenizers-0.20.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.46.3\n    Uninstalling transformers-4.46.3:\n      Successfully uninstalled transformers-4.46.3\nSuccessfully installed tokenizers-0.19.1 transformers-4.40.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Define file paths for train and test data\ntrain_file = \"/kaggle/input/lra-listops-reduced/train_d20s.tsv\"\ntest_file = \"/kaggle/input/lra-listops-reduced/test_d20s.tsv\"\n\n# Load the data\ntrain_df = pd.read_csv(train_file, sep=\"\\t\", header=0)\ntest_df = pd.read_csv(test_file, sep=\"\\t\", header=0)\n\n# Remove any possible header rows included as data\ntrain_df = train_df[train_df[\"Target\"] != \"Target\"]\ntest_df = test_df[test_df[\"Target\"] != \"Target\"]\n\n# Convert labels to integers\ntrain_df[\"Target\"] = train_df[\"Target\"].astype(int)\ntest_df[\"Target\"] = test_df[\"Target\"].astype(int)\n\n# Filter sequences by length\ntrain_df = train_df[train_df[\"Source\"].apply(len) < 1024]\ntest_df = test_df[test_df[\"Source\"].apply(len) < 1024]\n\n# Shuffle the training data\ntrain_df = train_df.sample(frac=1, random_state=42)\n\nprint(\"Training data example:\")\nprint(train_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:55:42.350700Z","iopub.execute_input":"2024-12-06T19:55:42.351189Z","iopub.status.idle":"2024-12-06T19:55:43.060158Z","shell.execute_reply.started":"2024-12-06T19:55:42.351149Z","shell.execute_reply":"2024-12-06T19:55:43.059245Z"}},"outputs":[{"name":"stdout","text":"Training data example:\n       Target                                             Source\n33140       2                 ( ( ( ( ( [MED 1 ) 7 ) 3 ) 2 ) ] )\n16844       2  ( ( ( ( [SM 4 ) 8 ) ( ( ( ( [MIN 0 ) ( ( ( ( [...\n50823       6  ( ( ( ( [MAX 6 ) ( ( ( ( [MED 2 ) 2 ) 7 ) ] ) ...\n73017       3  ( ( ( ( [MED 0 ) ( ( ( ( ( [MED ( ( ( ( ( ( [S...\n5116        6  ( ( ( [MIN 6 ) ( ( ( ( ( ( [MAX 0 ) 3 ) 9 ) ( ...\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class LRADataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels.astype(int)  # Ensure labels are integers\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item]\n\n        # Tokenize and encode the text\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:55:43.061217Z","iopub.execute_input":"2024-12-06T19:55:43.061505Z","iopub.status.idle":"2024-12-06T19:55:43.067907Z","shell.execute_reply.started":"2024-12-06T19:55:43.061479Z","shell.execute_reply":"2024-12-06T19:55:43.066789Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class LRADataModule(pl.LightningDataModule):\n    def __init__(self, train_df, test_df, tokenizer, max_len, batch_size):\n        super().__init__()\n        self.train_df = train_df\n        self.test_df = test_df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.batch_size = batch_size\n\n    def setup(self, stage=None):\n        self.train_dataset = LRADataset(\n            texts=self.train_df[\"Source\"].to_numpy(),\n            labels=self.train_df[\"Target\"].to_numpy(),\n            tokenizer=self.tokenizer,\n            max_len=self.max_len,\n        )\n        self.test_dataset = LRADataset(\n            texts=self.test_df[\"Source\"].to_numpy(),\n            labels=self.test_df[\"Target\"].to_numpy(),\n            tokenizer=self.tokenizer,\n            max_len=self.max_len,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:55:43.069577Z","iopub.execute_input":"2024-12-06T19:55:43.069850Z","iopub.status.idle":"2024-12-06T19:55:43.082306Z","shell.execute_reply.started":"2024-12-06T19:55:43.069826Z","shell.execute_reply":"2024-12-06T19:55:43.081508Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# Initialize a WordLevel tokenizer\ntokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n\n# Set the pre-tokenization strategy\ntokenizer.pre_tokenizer = Whitespace()\n\n# Prepare a trainer with special tokens\ntrainer = WordLevelTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n\n# Train the tokenizer on your text file\nwith open(\"listops_sequences.txt\", \"w\") as f:\n    for sequence in train_df[\"Source\"]:\n        f.write(sequence + \"\\n\")\n\ntokenizer.train([\"listops_sequences.txt\"], trainer)\n\n# Save the tokenizer\ntokenizer.save(\"custom_tokenizer.json\")\nprint(\"Custom tokenizer saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:55:43.083324Z","iopub.execute_input":"2024-12-06T19:55:43.083660Z","iopub.status.idle":"2024-12-06T19:55:44.740686Z","shell.execute_reply.started":"2024-12-06T19:55:43.083625Z","shell.execute_reply":"2024-12-06T19:55:44.739719Z"}},"outputs":[{"name":"stdout","text":"Custom tokenizer saved!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from torchmetrics.classification import Accuracy\n\nclass MEGAClassifier(pl.LightningModule):\n    def __init__(self, config, num_classes=10, lr=1e-3):\n        super().__init__()\n        self.model = MegaForSequenceClassification(config)\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.lr = lr\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        labels = batch[\"labels\"]\n\n        logits = self(input_ids, attention_mask)\n        loss = self.criterion(logits, labels)\n        preds = torch.argmax(logits, dim=1)\n\n        self.log(\"train_loss\", loss)\n        self.log(\"train_acc\", self.accuracy(preds, labels))\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        labels = batch[\"labels\"]\n\n        logits = self(input_ids, attention_mask)\n        loss = self.criterion(logits, labels)\n        preds = torch.argmax(logits, dim=1)\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.accuracy(preds, labels), prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n        return [optimizer], [scheduler]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:55:44.741861Z","iopub.execute_input":"2024-12-06T19:55:44.742696Z","iopub.status.idle":"2024-12-06T19:55:44.752535Z","shell.execute_reply.started":"2024-12-06T19:55:44.742656Z","shell.execute_reply":"2024-12-06T19:55:44.751543Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Initialize tokenizer and data\nMAX_LEN = 1024\nBATCH_SIZE = 128\nEPOCHS = 10\nLEARNING_RATE = 1e-3\n\nfrom transformers import PreTrainedTokenizerFast\n\n# Load the custom tokenizer\ntokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\ntokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\nprint(\"Custom tokenizer loaded!\")\n\n\n# Define data module\ndata_module = LRADataModule(train_df, test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n\n# Define model\nconfig = MegaConfig(\n    vocab_size=tokenizer.vocab_size,\n    hidden_size=16,\n    num_hidden_layers=4,\n    num_labels=10,\n    max_positions=MAX_LEN,\n    bidirectional=True,\n    is_decoder=False,\n    use_cache=False,\n    activation_function=\"silu\",\n    attention_activation_function=\"softmax\",\n    norm_type=\"layernorm\",\n    dropout=0.1,\n    attention_dropout=0.0,\n    weight_decay=0.01,\n)\nmodel = MEGAClassifier(config=config, num_classes=10, lr=LEARNING_RATE)\n\n# Initialize trainer\ntrainer = pl.Trainer(\n    max_epochs=EPOCHS,\n    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n    devices=torch.cuda.device_count() if torch.cuda.is_available() else 1,\n    log_every_n_steps=10,\n)\n\n# Train the model\ntrainer.fit(model, datamodule=data_module)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:55:44.753680Z","iopub.execute_input":"2024-12-06T19:55:44.754002Z","iopub.status.idle":"2024-12-06T20:42:04.590542Z","shell.execute_reply.started":"2024-12-06T19:55:44.753966Z","shell.execute_reply":"2024-12-06T20:42:04.589619Z"}},"outputs":[{"name":"stdout","text":"Custom tokenizer loaded!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"693275edd43a4d77bd6658af262f8c37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Save the trained model\noutput_dir = \"./mega_model_listops_reduced_best_pl/\"\nos.makedirs(output_dir, exist_ok=True)\nmodel.model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"Model and tokenizer saved to {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:42:15.651386Z","iopub.execute_input":"2024-12-06T20:42:15.651734Z","iopub.status.idle":"2024-12-06T20:42:15.667417Z","shell.execute_reply.started":"2024-12-06T20:42:15.651705Z","shell.execute_reply":"2024-12-06T20:42:15.666529Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved to ./mega_model_listops_reduced_best_pl/\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!ls -la /kaggle/working/lightning_logs/version_0/checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:44:25.758759Z","iopub.execute_input":"2024-12-06T20:44:25.759486Z","iopub.status.idle":"2024-12-06T20:44:26.788886Z","shell.execute_reply.started":"2024-12-06T20:44:25.759448Z","shell.execute_reply":"2024-12-06T20:44:26.787849Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"total 1320\ndrwxr-xr-x 2 root root    4096 Dec  6 20:42  .\ndrwxr-xr-x 3 root root    4096 Dec  6 20:00  ..\n-rw------- 1 root root 1341216 Dec  6 20:42 'epoch=9-step=6580.ckpt'\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Load the model and tokenizer\nmodel = MEGAClassifier.load_from_checkpoint(\"lightning_logs/version_0/checkpoints/epoch=9-step=6580.ckpt\", config=config)\ntokenizer = AutoTokenizer.from_pretrained(output_dir)\n\n# Move the model to the appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Prepare a sample sequence\nsequences = [\"( MIN 1 2 9)\", \"( SM 1 5 1 1 )\", \"( MAX 1 3 9 5 4 1 9 5 1)\"]\nencoding = tokenizer(\n    sequences,\n    add_special_tokens=True,\n    max_length=16,\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\",\n)\n\nprint(encoding)\n\n# Move input tensors to the same device as the model\ninput_ids = encoding[\"input_ids\"].to(device)\nattention_mask = encoding[\"attention_mask\"].to(device)\n\n# Perform inference\nmodel.eval()\nwith torch.no_grad():\n    logits = model(input_ids, attention_mask)\n    probabilities = torch.softmax(logits, dim=-1)\n    predictions = torch.argmax(probabilities, dim=-1)\n\n\n\nprint(f\"Predictions: {predictions.tolist()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:58:40.346995Z","iopub.execute_input":"2024-12-06T20:58:40.347678Z","iopub.status.idle":"2024-12-06T20:58:40.467119Z","shell.execute_reply.started":"2024-12-06T20:58:40.347645Z","shell.execute_reply":"2024-12-06T20:58:40.466240Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[ 5, 19, 10, 11, 12,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 5, 22, 10, 14, 10, 10,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 5, 21, 10,  9, 12, 14, 17, 10, 12, 14, 10,  6,  0,  0,  0,  0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\nPredictions: [0, 7, 8]\n","output_type":"stream"}],"execution_count":45}]}