{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff589561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:15.632686Z",
     "iopub.status.busy": "2024-12-05T20:45:15.632435Z",
     "iopub.status.idle": "2024-12-05T20:45:41.021972Z",
     "shell.execute_reply": "2024-12-05T20:45:41.021253Z"
    },
    "id": "bnXKpCNPf9Sk",
    "outputId": "ecc5d481-aa5b-4ea6-a956-2e49ddca5e12",
    "papermill": {
     "duration": 25.396873,
     "end_time": "2024-12-05T20:45:41.023836",
     "exception": false,
     "start_time": "2024-12-05T20:45:15.626963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.40.2\r\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (3.15.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.26.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (2.32.3)\r\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\r\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (4.66.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2024.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.40.2) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (2024.6.2)\r\n",
      "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.20.3\r\n",
      "    Uninstalling tokenizers-0.20.3:\r\n",
      "      Successfully uninstalled tokenizers-0.20.3\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.46.3\r\n",
      "    Uninstalling transformers-4.46.3:\r\n",
      "      Successfully uninstalled transformers-4.46.3\r\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.40.2\r\n"
     ]
    }
   ],
   "source": [
    "# Install the required version of transformers\n",
    "!pip install -U transformers==4.40.2\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import MegaConfig, MegaForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1067d5dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:41.036656Z",
     "iopub.status.busy": "2024-12-05T20:45:41.036274Z",
     "iopub.status.idle": "2024-12-05T20:45:41.958048Z",
     "shell.execute_reply": "2024-12-05T20:45:41.957172Z"
    },
    "id": "s2NiNMIVf9Sp",
    "outputId": "6bf37d1b-9cc5-44a8-b2bd-52bf9fce1cac",
    "papermill": {
     "duration": 0.930718,
     "end_time": "2024-12-05T20:45:41.959964",
     "exception": false,
     "start_time": "2024-12-05T20:45:41.029246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Target                                             Source\n",
      "0       6  ( ( ( ( ( [MAX ( ( ( ( ( ( [MED 4 ) 6 ) 6 ) 0 ...\n",
      "1       7   ( ( ( ( [SM ( ( ( [MED 6 ) 5 ) ] ) ) 1 ) 1 ) ] )\n",
      "2       4                 ( ( ( ( ( [MAX 3 ) 4 ) 3 ) 3 ) ] )\n",
      "3       0  ( ( ( ( ( [MIN 0 ) 0 ) ( ( ( [MAX 4 ) ( ( ( ( ...\n",
      "4       9  ( ( ( ( ( ( [SM ( ( ( ( [MIN 5 ) ( ( ( [MAX ( ...\n",
      "   Target                                             Source\n",
      "0       6  ( ( ( ( ( [MAX ( ( ( ( ( ( [MED 4 ) 6 ) 6 ) 0 ...\n",
      "1       7   ( ( ( ( [SM ( ( ( [MED 6 ) 5 ) ] ) ) 1 ) 1 ) ] )\n",
      "2       4                 ( ( ( ( ( [MAX 3 ) 4 ) 3 ) 3 ) ] )\n",
      "3       0  ( ( ( ( ( [MIN 0 ) 0 ) ( ( ( [MAX 4 ) ( ( ( ( ...\n",
      "4       9  ( ( ( ( ( ( [SM ( ( ( ( [MIN 5 ) ( ( ( [MAX ( ...\n",
      "Index(['Target', 'Source'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Define the file paths\n",
    "train_file = '/kaggle/input/lra-listops-reduced/train_d20s.tsv'\n",
    "test_file = '/kaggle/input/lra-listops-reduced/test_d20s.tsv'\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(train_file, sep='\\t', header=0)\n",
    "test_df = pd.read_csv(test_file, sep='\\t', header=0)\n",
    "# Preview the data\n",
    "print(train_df.head())\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(train_file, sep='\\t', header=0)\n",
    "test_df = pd.read_csv(test_file, sep='\\t', header=0)\n",
    "# Preview the data\n",
    "print(train_df.head())\n",
    "print(train_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6923330d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:41.971321Z",
     "iopub.status.busy": "2024-12-05T20:45:41.971033Z",
     "iopub.status.idle": "2024-12-05T20:45:41.975105Z",
     "shell.execute_reply": "2024-12-05T20:45:41.974291Z"
    },
    "id": "RoggwnV9f9Sq",
    "papermill": {
     "duration": 0.011656,
     "end_time": "2024-12-05T20:45:41.976746",
     "exception": false,
     "start_time": "2024-12-05T20:45:41.965090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return ''.join(str(text).split())\n",
    "\n",
    "# Apply the cleaning function to the 'Source' column\n",
    "#train_df['Source'] = train_df['Source'].apply(clean_text)\n",
    "#test_df['Source'] = test_df['Source'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bd2fe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:41.987519Z",
     "iopub.status.busy": "2024-12-05T20:45:41.987266Z",
     "iopub.status.idle": "2024-12-05T20:45:41.998568Z",
     "shell.execute_reply": "2024-12-05T20:45:41.997603Z"
    },
    "id": "rEG3i6q-f9Sr",
    "outputId": "10390f55-b653-41ff-fed5-466c7a71c427",
    "papermill": {
     "duration": 0.018508,
     "end_time": "2024-12-05T20:45:42.000248",
     "exception": false,
     "start_time": "2024-12-05T20:45:41.981740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Target                                             Source\n",
      "0       6  ( ( ( ( ( [MAX ( ( ( ( ( ( [MED 4 ) 6 ) 6 ) 0 ...\n",
      "1       7   ( ( ( ( [SM ( ( ( [MED 6 ) 5 ) ] ) ) 1 ) 1 ) ] )\n",
      "2       4                 ( ( ( ( ( [MAX 3 ) 4 ) 3 ) 3 ) ] )\n",
      "3       0  ( ( ( ( ( [MIN 0 ) 0 ) ( ( ( [MAX 4 ) ( ( ( ( ...\n",
      "4       9  ( ( ( ( ( ( [SM ( ( ( ( [MIN 5 ) ( ( ( [MAX ( ...\n",
      "73\n",
      "48\n",
      "34\n",
      "271\n",
      "501\n",
      "577\n",
      "2542\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove any possible header rows included as data\n",
    "train_df = train_df[train_df['Target'] != 'Target']\n",
    "test_df = test_df[test_df['Target'] != 'Target']\n",
    "\n",
    "# Convert labels to integers\n",
    "train_df['Target'] = train_df['Target'].astype(int)\n",
    "test_df['Target'] = test_df['Target'].astype(int)\n",
    "print(train_df.head())\n",
    "\n",
    "print(len(train_df['Source'][0]))\n",
    "print(len(train_df['Source'][1]))\n",
    "print(len(train_df['Source'][2]))\n",
    "print(len(train_df['Source'][3]))\n",
    "print(len(train_df['Source'][4]))\n",
    "print(len(train_df['Source'][5]))\n",
    "print(len(train_df['Source'][8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a563d398",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:42.011156Z",
     "iopub.status.busy": "2024-12-05T20:45:42.010862Z",
     "iopub.status.idle": "2024-12-05T20:45:42.060185Z",
     "shell.execute_reply": "2024-12-05T20:45:42.059212Z"
    },
    "id": "_lRw41IPf9Ss",
    "outputId": "a913a18a-8930-4ce7-99b3-b92b6884e116",
    "papermill": {
     "duration": 0.056669,
     "end_time": "2024-12-05T20:45:42.061832",
     "exception": false,
     "start_time": "2024-12-05T20:45:42.005163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sequence lengths:\n",
      "count    90000.000000\n",
      "mean       277.778200\n",
      "std        491.629979\n",
      "min          1.000000\n",
      "25%         49.000000\n",
      "50%         99.000000\n",
      "75%        259.000000\n",
      "max       7593.000000\n",
      "Name: Source, dtype: float64\n",
      "0     73\n",
      "1     48\n",
      "2     34\n",
      "3    271\n",
      "4    501\n",
      "Name: Source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Compute the lengths of the original sequences\n",
    "seqLengths= train_df['Source'].apply(lambda x: len(x))\n",
    "\n",
    "# Describe the sequence lengths\n",
    "print(\"Training data sequence lengths:\")\n",
    "print(seqLengths.describe())\n",
    "\n",
    "print(seqLengths.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70287f36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:42.072798Z",
     "iopub.status.busy": "2024-12-05T20:45:42.072545Z",
     "iopub.status.idle": "2024-12-05T20:45:42.164491Z",
     "shell.execute_reply": "2024-12-05T20:45:42.163570Z"
    },
    "id": "l3GY0_E7f9St",
    "outputId": "b2dbf190-0076-43c7-e477-55730679624d",
    "papermill": {
     "duration": 0.099383,
     "end_time": "2024-12-05T20:45:42.166155",
     "exception": false,
     "start_time": "2024-12-05T20:45:42.066772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sequence lengths:\n",
      "count    84162.000000\n",
      "mean       170.649367\n",
      "std        194.364638\n",
      "min          1.000000\n",
      "25%         48.000000\n",
      "50%         88.000000\n",
      "75%        207.000000\n",
      "max       1023.000000\n",
      "Name: Source, dtype: float64\n",
      "33140     34\n",
      "16844    483\n",
      "50823     55\n",
      "73017    292\n",
      "5116     121\n",
      "Name: Source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filter the training and test datasets based on the condition\n",
    "train_df = train_df[train_df['Source'].apply(lambda x: len(x)) < 1024]\n",
    "test_df = test_df[test_df['Source'].apply(lambda x: len(x)) < 1024]\n",
    "\n",
    "# Randomly sample 10% of the filtered train data\n",
    "train_df = train_df.sample(frac=1, random_state=42)  # Set random_state for reproducibility\n",
    "\n",
    "# Check the filtered dataframe\n",
    "seqLengths = train_df['Source'].apply(lambda x: len(x))\n",
    "\n",
    "# Describe the sequence lengths\n",
    "print(\"Training data sequence lengths:\")\n",
    "print(seqLengths.describe())\n",
    "\n",
    "print(seqLengths.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8eff494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:42.177809Z",
     "iopub.status.busy": "2024-12-05T20:45:42.177544Z",
     "iopub.status.idle": "2024-12-05T20:45:42.180935Z",
     "shell.execute_reply": "2024-12-05T20:45:42.180129Z"
    },
    "id": "UNpMPxwaf9St",
    "papermill": {
     "duration": 0.01097,
     "end_time": "2024-12-05T20:45:42.182567",
     "exception": false,
     "start_time": "2024-12-05T20:45:42.171597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc030da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:42.193663Z",
     "iopub.status.busy": "2024-12-05T20:45:42.193418Z",
     "iopub.status.idle": "2024-12-05T20:45:42.248453Z",
     "shell.execute_reply": "2024-12-05T20:45:42.247819Z"
    },
    "id": "epaeuELUf9Su",
    "papermill": {
     "duration": 0.06242,
     "end_time": "2024-12-05T20:45:42.250139",
     "exception": false,
     "start_time": "2024-12-05T20:45:42.187719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the sequences to a text file for tokenizer training\n",
    "with open(\"listops_sequences.txt\", \"w\") as f:\n",
    "    for sequence in train_df[\"Source\"]:\n",
    "        f.write(sequence + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b846629f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:42.261447Z",
     "iopub.status.busy": "2024-12-05T20:45:42.261193Z",
     "iopub.status.idle": "2024-12-05T20:45:43.870200Z",
     "shell.execute_reply": "2024-12-05T20:45:43.869424Z"
    },
    "id": "4sjbfMHNf9Sv",
    "papermill": {
     "duration": 1.616859,
     "end_time": "2024-12-05T20:45:43.872174",
     "exception": false,
     "start_time": "2024-12-05T20:45:42.255315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialize a WordLevel tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Set the pre-tokenization strategy\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Prepare a trainer with special tokens\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "\n",
    "# Train the tokenizer on your text file\n",
    "tokenizer.train([\"listops_sequences.txt\"], trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"custom_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80c3bb2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:43.884136Z",
     "iopub.status.busy": "2024-12-05T20:45:43.883859Z",
     "iopub.status.idle": "2024-12-05T20:45:43.889149Z",
     "shell.execute_reply": "2024-12-05T20:45:43.888275Z"
    },
    "id": "-zK6YG-Uf9Sv",
    "outputId": "45587c65-36f7-47ed-85f3-ad5416e148f6",
    "papermill": {
     "duration": 0.012864,
     "end_time": "2024-12-05T20:45:43.890776",
     "exception": false,
     "start_time": "2024-12-05T20:45:43.877912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=23, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the custom tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99ea37ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:43.902283Z",
     "iopub.status.busy": "2024-12-05T20:45:43.902062Z",
     "iopub.status.idle": "2024-12-05T20:45:43.907740Z",
     "shell.execute_reply": "2024-12-05T20:45:43.907011Z"
    },
    "id": "ZdJ7y6hcf9Sw",
    "papermill": {
     "duration": 0.013501,
     "end_time": "2024-12-05T20:45:43.909490",
     "exception": false,
     "start_time": "2024-12-05T20:45:43.895989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LRADataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels.astype(int)  # Ensure labels are integers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        # Tokenize and encode the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a36fb35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:43.920612Z",
     "iopub.status.busy": "2024-12-05T20:45:43.920397Z",
     "iopub.status.idle": "2024-12-05T20:45:43.928405Z",
     "shell.execute_reply": "2024-12-05T20:45:43.927536Z"
    },
    "id": "pRX285REf9Sw",
    "papermill": {
     "duration": 0.015425,
     "end_time": "2024-12-05T20:45:43.930100",
     "exception": false,
     "start_time": "2024-12-05T20:45:43.914675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = LRADataset(\n",
    "        texts=df['Source'].to_numpy(),\n",
    "        labels=df['Target'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=10\n",
    "    )\n",
    "\n",
    "# Parameters\n",
    "MAX_LEN = 1024\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Create data loaders\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a126ebdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:43.942010Z",
     "iopub.status.busy": "2024-12-05T20:45:43.941753Z",
     "iopub.status.idle": "2024-12-05T20:45:44.003255Z",
     "shell.execute_reply": "2024-12-05T20:45:44.002358Z"
    },
    "id": "i2eA5Lhgf9Sx",
    "outputId": "d10260f8-7d5d-4a7c-c8ad-8da0fd1f8654",
    "papermill": {
     "duration": 0.069515,
     "end_time": "2024-12-05T20:45:44.005036",
     "exception": false,
     "start_time": "2024-12-05T20:45:43.935521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84162\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "num_labels = train_df[\"Source\"].nunique()\n",
    "print(num_labels)\n",
    "print(tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef765798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:44.016906Z",
     "iopub.status.busy": "2024-12-05T20:45:44.016641Z",
     "iopub.status.idle": "2024-12-05T20:45:44.141625Z",
     "shell.execute_reply": "2024-12-05T20:45:44.140759Z"
    },
    "id": "O9vpnzKqf9Sx",
    "papermill": {
     "duration": 0.133131,
     "end_time": "2024-12-05T20:45:44.143725",
     "exception": false,
     "start_time": "2024-12-05T20:45:44.010594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of unique labels\n",
    "\n",
    "\n",
    "# Configure the MEGA model\n",
    "config = MegaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=16,\n",
    "    num_hidden_layers=4,\n",
    "    num_labels=10,\n",
    "    max_positions=MAX_LEN,\n",
    "    bidirectional=True,\n",
    "    is_decoder=False,\n",
    "    use_cache=False,\n",
    "    activation_function='silu',  # Match activation-fn parameter\n",
    "    attention_activation_function='softmax',  # Match attention-activation-fn parameter\n",
    "    norm_type='layernorm',  # Match norm-type parameter\n",
    "    dropout=0.1,  # Match dropout parameter\n",
    "    attention_dropout=0.0,  # Match attention-dropout parameter\n",
    "    weight_decay=0.01,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "model = MegaForSequenceClassification(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58c7818f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:44.159248Z",
     "iopub.status.busy": "2024-12-05T20:45:44.158961Z",
     "iopub.status.idle": "2024-12-05T20:45:44.928986Z",
     "shell.execute_reply": "2024-12-05T20:45:44.928257Z"
    },
    "id": "R-EhrN8Qf9Sy",
    "papermill": {
     "duration": 0.779474,
     "end_time": "2024-12-05T20:45:44.930848",
     "exception": false,
     "start_time": "2024-12-05T20:45:44.151374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Move the model to GPU(s)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)  #\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Optimizer and scheduler\n",
    "EPOCHS = 5\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps // 10,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "217215c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:44.943191Z",
     "iopub.status.busy": "2024-12-05T20:45:44.942757Z",
     "iopub.status.idle": "2024-12-05T20:45:44.951808Z",
     "shell.execute_reply": "2024-12-05T20:45:44.950982Z"
    },
    "id": "lBgjLgLbf9Sz",
    "papermill": {
     "duration": 0.016891,
     "end_time": "2024-12-05T20:45:44.953390",
     "exception": false,
     "start_time": "2024-12-05T20:45:44.936499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler\n",
    "):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids.cuda(),\n",
    "            attention_mask=attention_mask.cuda(),\n",
    "            labels=labels.cuda()\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    device\n",
    "):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids.cuda(),\n",
    "                attention_mask=attention_mask.cuda(),\n",
    "                labels=labels.cuda()\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cea513a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T20:45:44.965335Z",
     "iopub.status.busy": "2024-12-05T20:45:44.964834Z",
     "iopub.status.idle": "2024-12-05T21:08:33.932350Z",
     "shell.execute_reply": "2024-12-05T21:08:33.931026Z"
    },
    "id": "6j57p2O3f9Sz",
    "outputId": "f49cd757-75b3-467c-d725-00059ccfed91",
    "papermill": {
     "duration": 1368.975467,
     "end_time": "2024-12-05T21:08:33.934314",
     "exception": false,
     "start_time": "2024-12-05T20:45:44.958847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.168084220502152 accuracy 0.17078966754592334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 2.0057558803944975 accuracy 0.2056124626547162\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.9690379793883095 accuracy 0.2143366364867755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.9258948129576605 accuracy 0.2289799402475459\n",
      "Epoch 3/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.9083592715959057 accuracy 0.2486751740690573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.8687045525860142 accuracy 0.2645113102859582\n",
      "Epoch 4/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.8700348115257217 accuracy 0.26669993583802665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.8459887134062278 accuracy 0.2759282970550576\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.8552908792321805 accuracy 0.2743399634039115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.835294554362426 accuracy 0.279449423815621\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'train_acc': [],\n",
    "    'train_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        test_data_loader,\n",
    "        loss_fn,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "\n",
    "    history['train_acc'].append(train_acc.cpu().numpy())\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc.cpu().numpy())\n",
    "    history['val_loss'].append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4c9af2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T21:08:33.957814Z",
     "iopub.status.busy": "2024-12-05T21:08:33.956956Z",
     "iopub.status.idle": "2024-12-05T21:08:33.979235Z",
     "shell.execute_reply": "2024-12-05T21:08:33.978377Z"
    },
    "id": "o1Yjmz2_f9S0",
    "papermill": {
     "duration": 0.035363,
     "end_time": "2024-12-05T21:08:33.980698",
     "exception": false,
     "start_time": "2024-12-05T21:08:33.945335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./mega_model_listops_reduced_best_scheduled/\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory to save the model\n",
    "output_dir = './mega_model_listops_reduced_best_scheduled/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06c82e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T21:08:34.002886Z",
     "iopub.status.busy": "2024-12-05T21:08:34.002625Z",
     "iopub.status.idle": "2024-12-05T21:08:34.603238Z",
     "shell.execute_reply": "2024-12-05T21:08:34.601950Z"
    },
    "id": "1U63aQA6f9S0",
    "papermill": {
     "duration": 0.61325,
     "end_time": "2024-12-05T21:08:34.604575",
     "exception": true,
     "start_time": "2024-12-05T21:08:33.991325",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './mega_model_listops_best/'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './mega_model_listops_best/'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./mega_model_listops_best/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainedTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m MegaForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(output_dir)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2010\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[1;32m   2008\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   2009\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 2010\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2027\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2028\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:462\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './mega_model_listops_best/'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MegaForSequenceClassification, PreTrainedTokenizerFast\n",
    "\n",
    "# Specify the directory where the model and tokenizer are saved\n",
    "output_dir = './mega_model_listops_best/'\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(output_dir)\n",
    "\n",
    "# Load the model\n",
    "model = MegaForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94cd29",
   "metadata": {
    "id": "ualjgsyAf9S1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over sequences to tokenize them individually\n",
    "for sequence in sequences:\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    print(\"Tokens for sequence:\", sequence)\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Correct way to batch-encode sequences\n",
    "encoding = tokenizer(\n",
    "    sequences,\n",
    "    add_special_tokens=True,\n",
    "    max_length=model.config.max_length,  # Correct field for model's max length\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Move tensors to device\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "# Print for verification\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c14b87",
   "metadata": {
    "id": "ZrTA8qqef9S1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculations for inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "# The outputs contain logits\n",
    "logits = outputs.logits\n",
    "print(\"Input IDs shape:\", input_ids.shape)\n",
    "print(\"Attention mask shape:\", attention_mask.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0382cb01",
   "metadata": {
    "id": "BHzPRMSgf9S1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For batched input\n",
    "predicted_class_idx = torch.argmax(probabilities, dim=-1).cpu()  # Get indices for all examples\n",
    "print(f\"Predicted class indices: {predicted_class_idx.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584dd55c",
   "metadata": {
    "id": "6fM8r_mMf9S3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "sample_texts = [\n",
    "    \"( MIN 3 5 6 9 7 3 )\",\n",
    "    \"( MAX 2 4 6 )\",\n",
    "    \"( SUM 1 2 3 4 5 )\"\n",
    "]\n",
    "sample_labels = [0, 1, 2]  # Sample class labels\n",
    "\n",
    "# Dataset parameters\n",
    "max_len = 16\n",
    "\n",
    "# Create the dataset\n",
    "dataset = LRADataset(texts=sample_texts, labels=np.array(sample_labels), tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "# Inspect the first item in the dataset\n",
    "sample_item = dataset[0]\n",
    "\n",
    "print(\"Input IDs:\", sample_item['input_ids'])\n",
    "print(\"Attention Mask:\", sample_item['attention_mask'])\n",
    "print(\"Labels:\", sample_item['labels'])\n",
    "\n",
    "# Use a DataLoader to batch and inspect multiple items\n",
    "dataloader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# Get a batch\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "print(\"\\nBatch Input IDs:\\n\", batch['input_ids'])\n",
    "print(\"Batch Attention Mask:\\n\", batch['attention_mask'])\n",
    "print(\"Batch Labels:\\n\", batch['labels'])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6213643,
     "sourceId": 10079404,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6239759,
     "sourceId": 10113626,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1403.134258,
   "end_time": "2024-12-05T21:08:36.438515",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-05T20:45:13.304257",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
