{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-04T11:09:45.349758Z",
     "iopub.status.busy": "2024-12-04T11:09:45.348931Z",
     "iopub.status.idle": "2024-12-04T11:09:53.610068Z",
     "shell.execute_reply": "2024-12-04T11:09:53.609121Z",
     "shell.execute_reply.started": "2024-12-04T11:09:45.349718Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.40.2 in /opt/conda/lib/python3.10/site-packages (4.40.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.40.2) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.2) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# Install the required version of transformers\n",
    "!pip install -U transformers==4.40.2\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import MegaConfig, MegaForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:09:53.612199Z",
     "iopub.status.busy": "2024-12-04T11:09:53.611923Z",
     "iopub.status.idle": "2024-12-04T11:10:00.767126Z",
     "shell.execute_reply": "2024-12-04T11:10:00.766230Z",
     "shell.execute_reply.started": "2024-12-04T11:09:53.612172Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Source  Target\n",
      "0  ( ( ( ( ( ( ( ( ( ( ( [MIN 3 ) 7 ) ( ( ( ( ( (...       1\n",
      "1  ( ( ( ( ( [SM 3 ) 5 ) ( ( ( ( ( ( [MAX 4 ) ( (...       5\n",
      "2  ( ( ( ( ( ( ( [MED ( ( ( [MIN ( ( ( ( ( ( ( ( ...       5\n",
      "3  ( ( ( ( ( ( ( ( ( ( ( [MAX ( ( ( ( ( ( ( ( [ME...       9\n",
      "4  ( ( ( ( ( ( ( ( ( ( ( [SM ( ( ( [SM 4 ) 8 ) ] ...       5\n"
     ]
    }
   ],
   "source": [
    "# Define the file paths\n",
    "train_file = '/kaggle/input/lra-listops/basic_train.tsv'\n",
    "test_file = '/kaggle/input/lra-listops/basic_test.tsv'\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(train_file, sep='\\t', header=0)\n",
    "test_df = pd.read_csv(test_file, sep='\\t', header=0)\n",
    "# Preview the data\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:00.768279Z",
     "iopub.status.busy": "2024-12-04T11:10:00.768040Z",
     "iopub.status.idle": "2024-12-04T11:10:00.773043Z",
     "shell.execute_reply": "2024-12-04T11:10:00.772177Z",
     "shell.execute_reply.started": "2024-12-04T11:10:00.768255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return ''.join(str(text).split())\n",
    "\n",
    "# Apply the cleaning function to the 'Source' column\n",
    "#train_df['Source'] = train_df['Source'].apply(clean_text)\n",
    "#test_df['Source'] = test_df['Source'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:00.774938Z",
     "iopub.status.busy": "2024-12-04T11:10:00.774688Z",
     "iopub.status.idle": "2024-12-04T11:10:00.795614Z",
     "shell.execute_reply": "2024-12-04T11:10:00.794710Z",
     "shell.execute_reply.started": "2024-12-04T11:10:00.774906Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Source  Target\n",
      "0  ( ( ( ( ( ( ( ( ( ( ( [MIN 3 ) 7 ) ( ( ( ( ( (...       1\n",
      "1  ( ( ( ( ( [SM 3 ) 5 ) ( ( ( ( ( ( [MAX 4 ) ( (...       5\n",
      "2  ( ( ( ( ( ( ( [MED ( ( ( [MIN ( ( ( ( ( ( ( ( ...       5\n",
      "3  ( ( ( ( ( ( ( ( ( ( ( [MAX ( ( ( ( ( ( ( ( [ME...       9\n",
      "4  ( ( ( ( ( ( ( ( ( ( ( [SM ( ( ( [SM 4 ) 8 ) ] ...       5\n",
      "8538\n",
      "4289\n",
      "5445\n",
      "10036\n",
      "10834\n",
      "4785\n",
      "8390\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove any possible header rows included as data\n",
    "train_df = train_df[train_df['Target'] != 'Target']\n",
    "test_df = test_df[test_df['Target'] != 'Target']\n",
    "\n",
    "# Convert labels to integers\n",
    "train_df['Target'] = train_df['Target'].astype(int)\n",
    "test_df['Target'] = test_df['Target'].astype(int)\n",
    "print(train_df.head())\n",
    "\n",
    "print(len(train_df['Source'][0]))\n",
    "print(len(train_df['Source'][1]))\n",
    "print(len(train_df['Source'][2]))\n",
    "print(len(train_df['Source'][3]))\n",
    "print(len(train_df['Source'][4]))\n",
    "print(len(train_df['Source'][5]))\n",
    "print(len(train_df['Source'][8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:00.797051Z",
     "iopub.status.busy": "2024-12-04T11:10:00.796777Z",
     "iopub.status.idle": "2024-12-04T11:10:00.848068Z",
     "shell.execute_reply": "2024-12-04T11:10:00.847285Z",
     "shell.execute_reply.started": "2024-12-04T11:10:00.797026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sequence lengths:\n",
      "count    96000.000000\n",
      "mean      6609.981573\n",
      "std       2517.482889\n",
      "min       3171.000000\n",
      "25%       4483.000000\n",
      "50%       6086.000000\n",
      "75%       8368.000000\n",
      "max      12807.000000\n",
      "Name: Source, dtype: float64\n",
      "0     8538\n",
      "1     4289\n",
      "2     5445\n",
      "3    10036\n",
      "4    10834\n",
      "Name: Source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Compute the lengths of the original sequences\n",
    "seqLengths= train_df['Source'].apply(lambda x: len(x))\n",
    "\n",
    "# Describe the sequence lengths\n",
    "print(\"Training data sequence lengths:\")\n",
    "print(seqLengths.describe())\n",
    "\n",
    "print(seqLengths.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:00.849760Z",
     "iopub.status.busy": "2024-12-04T11:10:00.849161Z",
     "iopub.status.idle": "2024-12-04T11:10:00.955545Z",
     "shell.execute_reply": "2024-12-04T11:10:00.954736Z",
     "shell.execute_reply.started": "2024-12-04T11:10:00.849721Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sequence lengths:\n",
      "count    17150.000000\n",
      "mean      3634.528805\n",
      "std        260.173496\n",
      "min       3171.000000\n",
      "25%       3408.000000\n",
      "50%       3632.000000\n",
      "75%       3858.000000\n",
      "max       4095.000000\n",
      "Name: Source, dtype: float64\n",
      "9     3506\n",
      "12    3232\n",
      "14    3572\n",
      "21    3997\n",
      "24    3359\n",
      "Name: Source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filter the training and test datasets based on the condition\n",
    "train_df = train_df[train_df['Source'].apply(lambda x: len(x)) < 4096]\n",
    "test_df = test_df[test_df['Source'].apply(lambda x: len(x)) < 4096]\n",
    "\n",
    "# Randomly sample 10% of the filtered train data\n",
    "#train_df = train_df.sample(frac=, random_state=42)  \n",
    "\n",
    "# Check the filtered dataframe\n",
    "seqLengths = train_df['Source'].apply(lambda x: len(x))\n",
    "\n",
    "# Describe the sequence lengths\n",
    "print(\"Training data sequence lengths:\")\n",
    "print(seqLengths.describe())\n",
    "\n",
    "print(seqLengths.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:00.957018Z",
     "iopub.status.busy": "2024-12-04T11:10:00.956781Z",
     "iopub.status.idle": "2024-12-04T11:10:00.960482Z",
     "shell.execute_reply": "2024-12-04T11:10:00.959580Z",
     "shell.execute_reply.started": "2024-12-04T11:10:00.956993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:00.961592Z",
     "iopub.status.busy": "2024-12-04T11:10:00.961367Z",
     "iopub.status.idle": "2024-12-04T11:10:01.112854Z",
     "shell.execute_reply": "2024-12-04T11:10:01.111954Z",
     "shell.execute_reply.started": "2024-12-04T11:10:00.961557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the sequences to a text file for tokenizer training\n",
    "with open(\"listops_sequences.txt\", \"w\") as f:\n",
    "    for sequence in train_df[\"Source\"]:\n",
    "        f.write(sequence + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:01.114139Z",
     "iopub.status.busy": "2024-12-04T11:10:01.113884Z",
     "iopub.status.idle": "2024-12-04T11:10:07.654753Z",
     "shell.execute_reply": "2024-12-04T11:10:07.653686Z",
     "shell.execute_reply.started": "2024-12-04T11:10:01.114114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialize a WordLevel tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Set the pre-tokenization strategy\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Prepare a trainer with special tokens\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "\n",
    "# Train the tokenizer on your text file\n",
    "tokenizer.train([\"listops_sequences.txt\"], trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"custom_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:07.657958Z",
     "iopub.status.busy": "2024-12-04T11:10:07.657625Z",
     "iopub.status.idle": "2024-12-04T11:10:07.664848Z",
     "shell.execute_reply": "2024-12-04T11:10:07.663558Z",
     "shell.execute_reply.started": "2024-12-04T11:10:07.657926Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=23, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the custom tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:07.666299Z",
     "iopub.status.busy": "2024-12-04T11:10:07.666012Z",
     "iopub.status.idle": "2024-12-04T11:10:07.676628Z",
     "shell.execute_reply": "2024-12-04T11:10:07.675724Z",
     "shell.execute_reply.started": "2024-12-04T11:10:07.666270Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LRADataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels.astype(int)  # Ensure labels are integers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "  \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "        \n",
    "        # Tokenize and encode the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:07.678097Z",
     "iopub.status.busy": "2024-12-04T11:10:07.677728Z",
     "iopub.status.idle": "2024-12-04T11:10:07.690060Z",
     "shell.execute_reply": "2024-12-04T11:10:07.689292Z",
     "shell.execute_reply.started": "2024-12-04T11:10:07.678059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = LRADataset(\n",
    "        texts=df['Source'].to_numpy(),\n",
    "        labels=df['Target'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "  \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "MAX_LEN = 4096\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Create data loaders\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:07.691275Z",
     "iopub.status.busy": "2024-12-04T11:10:07.691024Z",
     "iopub.status.idle": "2024-12-04T11:10:07.836213Z",
     "shell.execute_reply": "2024-12-04T11:10:07.835514Z",
     "shell.execute_reply.started": "2024-12-04T11:10:07.691251Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17150\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "num_labels = train_df[\"Source\"].nunique()\n",
    "print(num_labels)\n",
    "print(tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:07.837413Z",
     "iopub.status.busy": "2024-12-04T11:10:07.837169Z",
     "iopub.status.idle": "2024-12-04T11:10:07.861807Z",
     "shell.execute_reply": "2024-12-04T11:10:07.860968Z",
     "shell.execute_reply.started": "2024-12-04T11:10:07.837388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = MegaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=16, \n",
    "    num_hidden_layers=6,\n",
    "    num_labels=10,\n",
    "    max_positions=MAX_LEN,\n",
    "    bidirectional=True,\n",
    "    is_decoder=False,\n",
    "    use_cache=False,\n",
    "    activation_function='silu',  # Match activation-fn parameter\n",
    "    attention_activation_function='softmax',  # Match attention-activation-fn parameter\n",
    "    norm_type='layernorm',  # Match norm-type parameter\n",
    "    dropout=0.1,  # Match dropout parameter\n",
    "    attention_dropout=0.0,  # Match attention-dropout parameter\n",
    "    weight_decay=0.01,\n",
    "    \n",
    ")\n",
    "\n",
    "model = MegaForSequenceClassification(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:07.863133Z",
     "iopub.status.busy": "2024-12-04T11:10:07.862878Z",
     "iopub.status.idle": "2024-12-04T11:10:07.879528Z",
     "shell.execute_reply": "2024-12-04T11:10:07.878927Z",
     "shell.execute_reply.started": "2024-12-04T11:10:07.863106Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Move the model to GPU(s)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)  # \n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps // 10,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:07.880449Z",
     "iopub.status.busy": "2024-12-04T11:10:07.880198Z",
     "iopub.status.idle": "2024-12-04T11:10:07.891267Z",
     "shell.execute_reply": "2024-12-04T11:10:07.890395Z",
     "shell.execute_reply.started": "2024-12-04T11:10:07.880421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model, \n",
    "    data_loader, \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    device, \n",
    "    scheduler\n",
    "):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "  \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "      \n",
    "        outputs = model(\n",
    "            input_ids=input_ids.cuda(),\n",
    "            attention_mask=attention_mask.cuda(),\n",
    "            labels=labels.cuda()\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "      \n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "  \n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(\n",
    "    model, \n",
    "    data_loader, \n",
    "    loss_fn, \n",
    "    device\n",
    "):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "          \n",
    "            outputs = model(\n",
    "                input_ids=input_ids.cuda(),\n",
    "                attention_mask=attention_mask.cuda(),\n",
    "                labels=labels.cuda()\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "          \n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "  \n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:10:07.892750Z",
     "iopub.status.busy": "2024-12-04T11:10:07.892415Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.247810343141431 accuracy 0.16466472303206997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 2.1037932137648263 accuracy 0.21465968586387435\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.090777186649059 accuracy 0.20956268221574345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 2.0760416065653167 accuracy 0.19633507853403143\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'train_acc': [],\n",
    "    'train_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "  \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model, \n",
    "        train_data_loader, \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler\n",
    "    )\n",
    "  \n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "  \n",
    "    val_acc, val_loss = eval_model(\n",
    "        model, \n",
    "        test_data_loader, \n",
    "        loss_fn, \n",
    "        device\n",
    "    )\n",
    "  \n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  \n",
    "    history['train_acc'].append(train_acc.cpu().numpy())\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc.cpu().numpy())\n",
    "    history['val_loss'].append(val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Specify the directory to save the model\n",
    "output_dir = './mega_model_listops_best_scheduled/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MegaForSequenceClassification, PreTrainedTokenizerFast\n",
    "\n",
    "output_dir = './mega_model_listops_best/'\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(output_dir)\n",
    "\n",
    "# Load the model\n",
    "model = MegaForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Iterate over sequences to tokenize them individually\n",
    "for sequence in sequences:\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    print(\"Tokens for sequence:\", sequence)\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Correct way to batch-encode sequences\n",
    "encoding = tokenizer(\n",
    "    sequences,\n",
    "    add_special_tokens=True,\n",
    "    max_length=model.config.max_length,  # Correct field for model's max length\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Move tensors to device\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "# Print for verification\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculations for inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "# The outputs contain logits\n",
    "logits = outputs.logits\n",
    "print(\"Input IDs shape:\", input_ids.shape)\n",
    "print(\"Attention mask shape:\", attention_mask.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# For batched input\n",
    "predicted_class_idx = torch.argmax(probabilities, dim=-1).cpu()  # Get indices for all examples\n",
    "print(f\"Predicted class indices: {predicted_class_idx.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "sample_texts = [\n",
    "    \"( MIN 3 5 6 9 7 3 )\",\n",
    "    \"( MAX 2 4 6 )\",\n",
    "    \"( SUM 1 2 3 4 5 )\"\n",
    "]\n",
    "sample_labels = [0, 1, 2]  # Sample class labels\n",
    "\n",
    "# Dataset parameters\n",
    "max_len = 16\n",
    "\n",
    "# Create the dataset\n",
    "dataset = LRADataset(texts=sample_texts, labels=np.array(sample_labels), tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "# Inspect the first item in the dataset\n",
    "sample_item = dataset[0]\n",
    "\n",
    "print(\"Input IDs:\", sample_item['input_ids'])\n",
    "print(\"Attention Mask:\", sample_item['attention_mask'])\n",
    "print(\"Labels:\", sample_item['labels'])\n",
    "\n",
    "# Use a DataLoader to batch and inspect multiple items\n",
    "dataloader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# Get a batch\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "print(\"\\nBatch Input IDs:\\n\", batch['input_ids'])\n",
    "print(\"Batch Attention Mask:\\n\", batch['attention_mask'])\n",
    "print(\"Batch Labels:\\n\", batch['labels'])\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6213643,
     "sourceId": 10079404,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
